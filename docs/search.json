[
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "The US Constitution sets the basic rules of electing the President in Section 1 of Article II, which we quote here in part:\nEach State shall appoint, in such Manner as the Legislature thereof may direct, a Number of Electors, equal to the whole Number of Senators and Representatives to which the State may be entitled in the Congress: but no Senator or Representative, or Person holding an Office of Trust or Profit under the United States, shall be appointed an Elector.\nThe Electors shall meet in their respective States, and vote by Ballot for two Persons, of whom one at least shall not be an Inhabitant of the same State with themselves. And they shall make a List of all the Persons voted for, and of the Number of Votes for each; which List they shall sign and certify, and transmit sealed to the Seat of the Government of the United States, directed to the President of the Senate. The President of the Senate shall, in the Presence of the Senate and House of Representatives, open all the Certificates, and the Votes shall then be counted. The Person having the greatest Number of Votes shall be the President, if such Number be a Majority of the whole Number of Electors appointed; and if there be more than one who have such Majority, and have an equal Number of Votes, then the House of Representatives shall immediately chuse by Ballot one of them for President; and if no Person have a Majority, then from the five highest on the List the said House shall in like Manner chuse the President. But in chusing the President, the Votes shall be taken by States, the Representation from each State having one Vote; A quorum for this Purpose shall consist of a Member or Members from two thirds of the States, and a Majority of all the States shall be necessary to a Choice. In every Case, after the Choice of the President, the Person having the greatest Number of Votes of the Electors shall be the Vice President. But if there should remain two or more who have equal Votes, the Senate shall chuse from them by Ballot the Vice President. Though the details have varied over time due to amendment, statue, and technology, this basic outline of this allocation scheme remains unchanged:\nEach state gets electoral college votes, where is the number of Representatives that state has in the US House of Representatives. In this mini-project, you can use the number of districts in a state to determine the number of congressional representatives (one per district). States can allocate those votes however they wish The president is the candidate who receives a majority of electoral college votes Notably, the Constitution sets essentially no rules on how the electoral college votes (ECVs) for a particular state are allocated. At different points in history, different states have elected to use each of the following:\nDirect allocation of ECVs by state legislature (no vote) Allocation of all ECVs to winner of state-wide popular vote Allocation of all ECVs to winner of nation-wide popular vote Allocation of ECVs to popular vote winner by congressional district + allocation of remaining ECVs to the state-wide popular vote winner Currently, only Maine and Nebraska use the final option; the other 48 states and the District of Columbia award all ECVs to the winner of their state-wide popular vote. We emphasize here that “statewide winner-take-all” is a choice made by the individual states, not dictated by the US constitution, and that states have the power to change it should they wish.1\nTo my knowledge, no US state uses true proportionate state-wide representation, though I believe such a ECV-allocation scheme would be consistent with the US Constitution. For example, if a state with 5 ECVs had 60,000 votes for Candidate A and 40,000 cast for Candidate B, it could award 3 ECVs to A and 2 to B, regardless of the spatial distribution of those votes within the state."
  },
  {
    "objectID": "mp03.html#data-i-us-house-election-votes-from-1976-to-2022",
    "href": "mp03.html#data-i-us-house-election-votes-from-1976-to-2022",
    "title": "Mini Project 3",
    "section": "Data I: US House Election Votes from 1976 to 2022",
    "text": "Data I: US House Election Votes from 1976 to 2022\nThe MIT Election Data Science Lab collects votes from all biennial congressional races in all 50 states here. Download this data as a CSV file using your web browser. Note that you will need to provide your contact info and agree to cite this data set in your final report.\nAdditionally, download statewide presidential vote counts from 1976 to 2022 here. As before, it will likely be easiest to download this data by hand using your web browser."
  },
  {
    "objectID": "mp03.html#data-ii-congressional-boundary-files-1976-to-2012",
    "href": "mp03.html#data-ii-congressional-boundary-files-1976-to-2012",
    "title": "Mini Project 3",
    "section": "Data II: Congressional Boundary Files 1976 to 2012",
    "text": "Data II: Congressional Boundary Files 1976 to 2012\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis have created shapefiles for all US congressional districts from 1789 to 2012; they generously make these available here.\nWe are going to download those with the following code:\n\n\nCode\n# Task 1: import congressional district data from cdmaps from 1976 to 2012\nget_cdmaps_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname_ext &lt;- paste0(fname, \".zip\")\n  if (!file.exists(fname_ext)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL,\n                  destfile = fname_ext\n    )\n  }\n}\noptions(timeout = 180) # keep the downloads from potentially timing out\n\nget_cdmaps_file(\"districts112\") # January 5, 2011 to January 3, 2013\nget_cdmaps_file(\"districts111\") # January 6, 2009 to December 22, 2010\nget_cdmaps_file(\"districts110\") # January 4, 2007 to January 3, 2009\nget_cdmaps_file(\"districts109\") # January 4, 2005 to December 9, 2006\nget_cdmaps_file(\"districts108\") # January 7, 2003 to December 8, 2004\nget_cdmaps_file(\"districts107\") # January 3, 2001 to November 22, 2002\nget_cdmaps_file(\"districts106\") # January 6, 1999 to December 15, 2000\nget_cdmaps_file(\"districts105\") # January 7, 1997 to December 19, 1998\nget_cdmaps_file(\"districts104\") # January 4, 1995 to October 4, 1996\nget_cdmaps_file(\"districts103\") # January 5, 1993 to December 1, 1994 \nget_cdmaps_file(\"districts102\") # January 3, 1991 to October 9, 1992\nget_cdmaps_file(\"districts101\") # January 3, 1989 to October 28, 1990\nget_cdmaps_file(\"districts100\") # January 6, 1987 to October 22, 1988\nget_cdmaps_file(\"districts099\")  # January 3, 1985 to October 18, 1986\nget_cdmaps_file(\"districts098\")  # January 3, 1983 to October 12, 1984\nget_cdmaps_file(\"districts097\")  # January 5, 1981 to December 23, 1982\nget_cdmaps_file(\"districts096\")  # January 15, 1979 to December 16, 1980\nget_cdmaps_file(\"districts095\")  # January 4, 1977 to October 15, 1978\nget_cdmaps_file(\"districts094\")  # January 14, 1975 to October 1, 1976"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "This mini project walks you through how to prepare, clean, and analyze data. The project will be followed by a report on the subject analyzed.\nWe are going to analyze the fiscal characteristics of major US public transit systems using data from the National Transit Database. The goal is to understand farebox recovery rates, analyze ridership, and examine the financial efficiency of various transit systems.\nWe will use data from the National Transit Database as our primary source. In particular, since we want to analyze farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source, we will need to analyze several different tables:\n\nThe 2022 Fare Revenue table\nThe latest Monthly Ridership tables\nThe 2022 Operating Expenses reports\n\nLet’s start by cleaning the data.\n\n\nThe following code will download, clean, and join the tables.\n\n# Load required libraries\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif(!require(\"readxl\")) install.packages(\"readxl\")\n\nLoading required package: readxl\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nFinally, let’s extract monthly transit numbers:\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\nThis creates a table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\nThe following code will rename the column ‘UZA Name’ to ‘metro_area’. Because it has no spaces in it, this name will be easier to manipulate in code.\n\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = `UZA Name`)\n\n\n\n\nThe ‘Mode’ column is also helpful, but it uses a set of codes that aren’t interpretable. To make life easier for ourselves, let’s use a case_when statement to transform this into something we can make sense of.\nFirst, let’s find the unique ‘Mode’ codes in our data using the distinct function. After examining the NTD website and finding the interpretations of these codes, we can complete the following snippet to recode the ‘Mode’ column.\n\n# Find unique Mode codes in the USAGE table\ndistinct(USAGE, Mode)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",       # HR: Heavy Rail\n    Mode == \"LR\" ~ \"Light Rail\",        # LR: Light Rail\n    Mode == \"MB\" ~ \"Bus\",               # MB: Bus (Motor Bus)\n    Mode == \"CR\" ~ \"Commuter Rail\",     # CR: Commuter Rail\n    Mode == \"DR\" ~ \"Demand Response\",   # DR: Demand Response\n    Mode == \"VP\" ~ \"Vanpool\",           # VP: Vanpool\n    Mode == \"AR\" ~ \"Alaska Railroad\",   # AR: Alaska Railroad\n    Mode == \"RB\" ~ \"Bus Rapid Transit\", # RB: Bus Rapid Transit\n    Mode == \"FB\" ~ \"Ferryboat\",       # FB: Ferryboat\n    Mode == \"SR\" ~ \"Streetcar Rail\",       # SR: Streetcar Rail\n    Mode == \"TB\" ~ \"Trolleybus\",       # TB: Trolleybus\n    Mode == \"CB\" ~ \"Commuter Bus\",       # CB: Commuter Bus\n    Mode == \"YR\" ~ \"Hybrid Rail\",       # YR: Hybrid Rail\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",  # MG: Monorail and Automated Guideway modes\n    Mode == \"TR\" ~ \"Aerial Tramway\",       # TR: Aerial Tramway\n    Mode == \"IP\" ~ \"Inclined Plane\",       # IP: Inclined Plane\n    Mode == \"PB\" ~ \"Publico\",       # PB: Publico\n    Mode == \"CC\" ~ \"Cable Car\",       # CC: Cable Car\n    TRUE ~ \"Unknown\"                    # Any other mode\n  ))\n\nNow that the data is clean, we can create an attractive summary table of the cleaned up USAGE table using the following snippet:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\n\nNow let’s analyse our data.\n\n\nUsing functions filter, group_by, summarize, arrange, we are going to answer the following questions for our analysis with the following codes:\n\nWhat transit agency had the most total VRM in our data set?\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(knitr)\n# Calculate total VRM by agency and find the agency with the most total VRM\nagency_most_vrm &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)  # Get the top agency\n# Display the result in a table\nkable(agency_most_vrm, caption = \"Transit Agency with Most Total VRM\")\n\n\nTransit Agency with Most Total VRM\n\n\nAgency\ntotal_VRM\n\n\n\n\nMTA New York City Transit\n10832855350\n\n\n\n\n\nThe transit agency that had the most total VRM in our data set is the MTA New York City Transit, which reported a total of 10,832,855,350 VRM.\n\nWhat transit mode had the most total VRM in our data set?\n\n\nmode_most_vrm &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)  # Get the top mode\n\nkable(mode_most_vrm, caption = \"Transit Mode with Most Total VRM\")\n\n\nTransit Mode with Most Total VRM\n\n\nMode\ntotal_VRM\n\n\n\n\nBus\n49444494088\n\n\n\n\n\nThe transit mode that had the most total VRM in our data set is the Bus with a total of 49,444,494,088 VRM.\n\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\n\n# Filter for Heavy Rail in NYC for May 2024\nNYC_Subway_trip_may_24 &lt;- USAGE %&gt;%\n  mutate(month = format(ymd(month), \"%Y-%m\")) %&gt;% \n  filter(Mode == \"Heavy Rail\", \n         Agency == \"MTA New York City Transit\",  \n         month == \"2024-05\") %&gt;%  \n  summarize(total_trip = sum(UPT, na.rm = TRUE)) \n\nkable(NYC_Subway_trip_may_24, caption = \"Total Heavy Rail Trips in NYC for May 2024\")\n\n\nTotal Heavy Rail Trips in NYC for May 2024\n\n\ntotal_trip\n\n\n\n\n180458819\n\n\n\n\n\nIn May 2024, 180,458,819 trips were taken on the NYC Subway (Heavy Rail).\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n# NYC Heavy Rail ridership for April 2019\nnyc_ridership_2019 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarise(nyc_sub_2019 = sum(UPT, na.rm = TRUE))\n\n# NYC Heavy Rail ridership for April 2020\nnyc_ridership_2020 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarise(nyc_subway_ridership_fall = sum(UPT, na.rm = TRUE))\n\n# Calculate the percentage change in ridership\nnyc_ridership &lt;- ((nyc_ridership_2020$nyc_subway_ridership_fall - nyc_ridership_2019$nyc_sub_2019) / nyc_ridership_2019$nyc_sub_2019) * 100\n\n# Combine results into a data frame for display\nresults &lt;- data.frame(\n  Year = c(\"April 2019\", \"April 2020\", \"Percentage Change\"),\n  UPT = c(nyc_ridership_2019$nyc_sub_2019, nyc_ridership_2020$nyc_subway_ridership_fall, nyc_ridership)\n)\n\nkable(results, caption = \"NYC Heavy Rail Ridership Comparison for April 2019 and April 2020\")\n\n\nNYC Heavy Rail Ridership Comparison for April 2019 and April 2020\n\n\nYear\nUPT\n\n\n\n\nApril 2019\n2.406554e+08\n\n\nApril 2020\n2.070482e+07\n\n\nPercentage Change\n-9.139649e+01\n\n\n\n\n\nHere our results show us that the NYC Subway ridership went from 240,655,437 in April 2019 to 20,704,824 in April 2020.Our last piece of the code informs us that this represent a 91% ridership fall between April 2019 and April 2020.\n\n\n\n\nFind the city with the most trips in a single month.\n\n\n# Calculate the highest total UPT by metro area and month\nhighest_upt &lt;- USAGE |&gt; \n  group_by(metro_area, month) |&gt; \n  summarize(total_UPT = sum(UPT, na.rm = TRUE), .groups = 'drop') |&gt; \n  slice_max(total_UPT, n = 1)  # Get the highest total UPT for each metro area and month\n\nkable(highest_upt, caption = \"Metro Area and Month with the Highest Total UPT\")\n\n\nMetro Area and Month with the Highest Total UPT\n\n\nmetro_area\nmonth\ntotal_UPT\n\n\n\n\nNew York–Jersey City–Newark, NY–NJ\n2014-10-01\n396079939\n\n\n\n\n\n\nWhich agency has the most extensive bus network based on VRM?\n\n\n# Calculate the agency with the highest total VRM for Motor Bus\nhighest_vrm_motor_bus &lt;- USAGE |&gt; \n  filter(Mode == \"Bus\") |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)\n\nkable(highest_vrm_motor_bus, caption = \"Agency with the Highest Total VRM for Motor Bus\")\n\n\nAgency with the Highest Total VRM for Motor Bus\n\n\nAgency\ntotal_VRM\n\n\n\n\nNew Jersey Transit Corporation\n3781858802\n\n\n\n\n\n\nHow did overall ridership change over time from 2022 to 2024?\n\n\n# Calculate the total UPT by year\nannual_upt &lt;- USAGE |&gt; \n  group_by(year = year(month)) |&gt; \n  summarize(total_UPT = sum(UPT, na.rm = TRUE)) |&gt; \n  arrange(year)\n\nkable(annual_upt, caption = \"Total UPT by Year\")\n\n\nTotal UPT by Year\n\n\nyear\ntotal_UPT\n\n\n\n\n2002\n8416364775\n\n\n2003\n8260528905\n\n\n2004\n8631243464\n\n\n2005\n8944272233\n\n\n2006\n9286575204\n\n\n2007\n9473412104\n\n\n2008\n9888947626\n\n\n2009\n9544171711\n\n\n2010\n9541689057\n\n\n2011\n9732100704\n\n\n2012\n10014964538\n\n\n2013\n10148119047\n\n\n2014\n10281027641\n\n\n2015\n10069594700\n\n\n2016\n9884467841\n\n\n2017\n9664591370\n\n\n2018\n9570957943\n\n\n2019\n9630231947\n\n\n2020\n4488461287\n\n\n2021\n4668753846\n\n\n2022\n6052448631\n\n\n2023\n6972821886\n\n\n2024\n4181081901\n\n\n\n\n\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\nNTD ID Agency metro_area Mode UPT VRM Make sure to ungroup your table after creating it.\nWe will name this table USAGE_2022_ANNUAL.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, `metro_area`, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm = TRUE),\n    VRM = sum(VRM, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'metro_area'. You can\noverride using the `.groups` argument.\n\nUSAGE_2022_ANNUAL\n\n# A tibble: 1,141 × 6\n   `NTD ID` Agency                                metro_area Mode     UPT    VRM\n      &lt;int&gt; &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1        1 King County                           Seattle--… Bus   5.40e7 6.16e7\n 2        1 King County                           Seattle--… Dema… 6.63e5 1.29e7\n 3        1 King County                           Seattle--… Ferr… 4.00e5 5.12e4\n 4        1 King County                           Seattle--… Stre… 1.12e6 1.80e5\n 5        1 King County                           Seattle--… Trol… 9.58e6 2.64e6\n 6        1 King County                           Seattle--… Vanp… 7.03e5 4.41e6\n 7        2 Spokane Transit Authority             Spokane, … Bus   6.60e6 6.49e6\n 8        2 Spokane Transit Authority             Spokane, … Dema… 3.10e5 4.04e6\n 9        2 Spokane Transit Authority             Spokane, … Vanp… 9.06e4 9.06e5\n10        3 Pierce County Transportation Benefit… Seattle--… Bus   4.95e6 4.23e6\n# ℹ 1,131 more rows\n\n\nOnce we have created this new table, we can merge it to the FINANCIALS data but first we need to make sure they have the same “mode” refrence names with the following code:\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",       # HR: Heavy Rail\n    Mode == \"LR\" ~ \"Light Rail\",        # LR: Light Rail\n    Mode == \"MB\" ~ \"Bus\",               # MB: Bus (Motor Bus)\n    Mode == \"CR\" ~ \"Commuter Rail\",     # CR: Commuter Rail\n    Mode == \"DR\" ~ \"Demand Response\",   # DR: Demand Response\n    Mode == \"VP\" ~ \"Vanpool\",           # VP: Vanpool\n    Mode == \"AR\" ~ \"Alaska Railroad\",   # AR: Alaska Railroad\n    Mode == \"RB\" ~ \"Bus Rapid Transit\", # RB: Bus Rapid Transit\n    Mode == \"FB\" ~ \"Ferryboat\",       # FB: Ferryboat\n    Mode == \"SR\" ~ \"Streetcar Rail\",       # SR: Streetcar Rail\n    Mode == \"TB\" ~ \"Trolleybus\",       # TB: Trolleybus\n    Mode == \"CB\" ~ \"Commuter Bus\",       # CB: Commuter Bus\n    Mode == \"YR\" ~ \"Hybrid Rail\",       # YR: Hybrid Rail\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",  # MG: Monorail and Automated Guideway modes\n    Mode == \"TR\" ~ \"Aerial Tramway\",       # TR: Aerial Tramway\n    Mode == \"IP\" ~ \"Inclined Plane\",       # IP: Inclined Plane\n    Mode == \"PB\" ~ \"Publico\",       # PB: Publico\n    Mode == \"CC\" ~ \"Cable Car\",       # CC: Cable Car\n    TRUE ~ \"Unknown\"                    # Any other mode\n  ))\n\nOnce we have made sure that USAGE_2022_ANNUAL and FINANCIALS have the same value for “mode”, we can merge USAGE_2022_ANNUAL to the FINANCIALS data as follows:\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\nUSAGE_AND_FINANCIALS |&gt;\n      DT::datatable()\n\n\n\n\n\n\n\n\nUsing the USAGE_AND_FINANCIALS table, we are going to try to answer the following questions:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of - Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\nWe will restrict our answers to major transit systems by defining them as those with 400,000 UPT per annum.\nTo answer these questions, we’ll need to perform the following steps using the USAGE_AND_FINANCIALS table:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\n\n\n# Find the transit system with the most UPT in 2022\nlibrary(dplyr)\nlibrary(knitr)\nmax_upt &lt;- USAGE_AND_FINANCIALS %&gt;%\n  filter(UPT &gt;= 400000) %&gt;%\n  arrange(desc(UPT)) %&gt;%\n  slice(1) %&gt;%\n  select(Agency, Mode, UPT)\nkable(max_upt, caption = \"Transit System with Most UPT in 2022\")\n\n\nTransit System with Most UPT in 2022\n\n\nAgency\nMode\nUPT\n\n\n\n\nMTA New York City Transit\nHeavy Rail\n1793073801\n\n\n\n\n\nThe answer shows us that the MTA New York City Transit is the transit system that had the most UTP with more than 1.79 billion of trips.\n\nWhich transit system (agency and mode) had the highest farebox recovery (Total Fares to Expenses ratio)?\n\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_fares = sum(`Total Fares`, na.rm = TRUE),\n    total_expenses = sum(Expenses, na.rm = TRUE)\n  ) |&gt;\n  mutate(farebox_recovery_ratio = total_fares / total_expenses) |&gt;\n  ungroup() |&gt;\n  arrange(desc(farebox_recovery_ratio)) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(highest_farebox_recovery, caption = \"Transit System with Highest Farebox Recovery Ratio\")\n\n\nTransit System with Highest Farebox Recovery Ratio\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_fares\ntotal_expenses\nfarebox_recovery_ratio\n\n\n\n\nTransit Authority of Central Kentucky\nVanpool\n97300\n40801\n2.384746\n\n\n\n\n\nThe Transit System with the highest recovery ratio is the Transit Authority of Central Kentuchy, with the Vanpool mode and a recovery ratio of $2.38 USD.\n3.Which transit system (agency and mode) has the lowest expenses per UPT?\n\nlowest_expenses_per_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_expenses = sum(Expenses, na.rm = TRUE),\n    total_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  filter(total_UPT &gt;= 400000) |&gt;\n  mutate(expenses_per_UPT = total_expenses / total_UPT) |&gt;\n  ungroup() |&gt;\n  arrange(expenses_per_UPT) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(lowest_expenses_per_upt, caption = \"Transit System with Lowest Expenses per UPT\")\n\n\nTransit System with Lowest Expenses per UPT\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_expenses\ntotal_UPT\nexpenses_per_UPT\n\n\n\n\nNorth Carolina State University\nBus\n2727412\n2313091\n1.17912\n\n\n\n\n\nNorth Carolina State University is the agency with the lowest expenses per UPT with the Bus mode. The expenses per UPT is $1.18 USD.\n4.Which transit system (agency and mode) has the highest total fares per UPT?\n\nhighest_fares_per_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_fares1 = sum(`Total Fares`, na.rm = TRUE),\n    total_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  mutate(total_fares_per_UPT = total_fares1 / total_UPT) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_fares_per_UPT)) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(highest_fares_per_upt, caption = \"Transit System with Highest Total Fares per UPT\")\n\n\nTransit System with Highest Total Fares per UPT\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_fares1\ntotal_UPT\ntotal_fares_per_UPT\n\n\n\n\nAltoona Metro Transit\nDemand Response\n17163\n26\n660.1154\n\n\n\n\n\nThe Altoona Metro Transit with the Demand Response mode had the highest total fares per UPT. With just 26 trips, it collected a total in fares of $17,058 USD with a fare of $660 USD per UPT.\n5.Which transit system (agency and mode) has the lowest expenses per VRM?\n\nlowest_expenses_per_vrm &lt;- USAGE_AND_FINANCIALS %&gt;%\n  filter(UPT &gt;= 400000) %&gt;%\n  mutate(expenses_per_vrm = Expenses / VRM) %&gt;%\n  arrange(expenses_per_vrm) %&gt;%\n  slice(1) %&gt;%\n  select(Agency, Mode, expenses_per_vrm)\n\nkable(lowest_expenses_per_vrm, caption = \"Transit System with Lowest Expenses per VRM in 2022\")\n\n\nTransit System with Lowest Expenses per VRM in 2022\n\n\nAgency\nMode\nexpenses_per_vrm\n\n\n\n\nMetropolitan Transportation Commission\nVanpool\n0.4449998\n\n\n\n\n\nNew Mexico Department of Transportation Vanpool mode has the lowest expense per VRM. Its fare per VRM is around 0.45.\n6.Which transit system (agency and mode) has the highest total fares per VRM?\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS %&gt;%\n  mutate(fares_per_VRM = `Total Fares` / VRM)  %&gt;%  \n  arrange(desc(fares_per_VRM)) %&gt;%  \n  slice_max(fares_per_VRM, n = 1) \n\nkable(highest_fares_per_VRM, caption = \"Transit System with Highest Fares per VRM in 2022\")\n\n\nTransit System with Highest Fares per VRM in 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\nAgency Name\nTotal Fares\nExpenses\nfares_per_VRM\n\n\n\n\n50521\nChicago Water Taxi (Wendella)\nChicago, IL–IN\nFerryboat\n16936\n600\nChicago Water Taxi (Wendella)\n142473\n211296\n237.455\n\n\n\n\n\nThe transit system that has the highest total fares per VRM is the Ferryboat from Chicago Water Taxi (Wendella) with a total fare per VRM of $237 USD.\n\n\n\n\nThis mini project provides several key insights into the fiscal characteristics, ridership, and operational efficiency of major U.S. public transit systems in 2022, based on data from the National Transit Database. Below are the major conclusions derived from each analysis:\n\nMost Total Vehicle Revenue Miles (VRM)\n\nTransit Agency: The MTA New York City Transit had the highest total Vehicle Revenue Miles (VRM) with over 10.8 billion VRM. Transit Mode: Buses had the most total VRM, accumulating more than 49.4 billion VRM across agencies. This indicates that the MTA operates the largest transit network in terms of vehicle miles, reflecting its role in serving a massive, densely populated area like New York City. Buses are crucial across transit systems, covering extensive distances.\n\nRidership Insights\n\nNYC Subway Ridership: In May 2024, there were approximately 180 million trips taken on the NYC Subway (Heavy Rail), showing the continued importance of the subway in daily transportation. Ridership Decline: NYC Subway ridership fell drastically between April 2019 and April 2020 by 91%, primarily due to the COVID-19 pandemic, reflecting the severe impact of external shocks like pandemics on public transit usage. This emphasizes how ridership levels can be highly volatile in response to global events, such as health crises, with significant consequences for transit agencies’ revenue and service planning.\n\nHighest Ridership in a Single Month\n\nMetro Area: New York–Jersey City–Newark, NY–NJ had the highest number of trips in a single month (October 2014), with nearly 396 million unlinked passenger trips (UPT). This reinforces New York City’s dominance as the leading metropolitan area in the U.S. in terms of public transit usage, particularly for heavy rail and bus systems.\n\nBus Network Extent\n\nAgency with Most Extensive Bus Network: New Jersey Transit Corporation had the most extensive bus network based on VRM, with over 3.78 billion VRM, reflecting its widespread service area and role in connecting commuters across state lines.\n\nRidership Trends (2022-2024)\n\nRidership saw a gradual recovery post-pandemic, with total UPT increasing from 6 billion in 2022 to nearly 7 billion in 2023. However, it significantly dropped again in 2024 to just over 4.18 billion, potentially indicating further external factors or a slow post-pandemic recovery.\n\nAnnual UPT and VRM for 2022\n\nThe analysis of UPT and VRM by transit mode and metro area in 2022 highlights regional differences in transit usage and network extent. For instance, King County (Seattle) had significant bus usage and VRM.\n\nFarebox Recovery and Financial Efficiency\n\nHighest Farebox Recovery: The Transit Authority of Central Kentucky had the highest farebox recovery ratio at 2.38, meaning the system collected more in fares than it spent on expenses (a rare occurrence in public transit). Lowest Expenses per UPT: A yet-to-be-confirmed agency/mode from the final analysis had the lowest expenses per UPT, reflecting operational efficiency in terms of cost per passenger. These insights into farebox recovery and financial efficiency help identify which systems are more sustainable and cost-effective, and which may require additional subsidies or fare increases to maintain financial stability."
  },
  {
    "objectID": "mp01.html#data-preparation",
    "href": "mp01.html#data-preparation",
    "title": "Mini Project 1",
    "section": "",
    "text": "The following code will download, clean, and join the tables.\n\n# Load required libraries\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif(!require(\"readxl\")) install.packages(\"readxl\")\n\nLoading required package: readxl\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nFinally, let’s extract monthly transit numbers:\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\nThis creates a table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\nThe following code will rename the column ‘UZA Name’ to ‘metro_area’. Because it has no spaces in it, this name will be easier to manipulate in code.\n\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = `UZA Name`)\n\n\n\n\nThe ‘Mode’ column is also helpful, but it uses a set of codes that aren’t interpretable. To make life easier for ourselves, let’s use a case_when statement to transform this into something we can make sense of.\nFirst, let’s find the unique ‘Mode’ codes in our data using the distinct function. After examining the NTD website and finding the interpretations of these codes, we can complete the following snippet to recode the ‘Mode’ column.\n\n# Find unique Mode codes in the USAGE table\ndistinct(USAGE, Mode)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",       # HR: Heavy Rail\n    Mode == \"LR\" ~ \"Light Rail\",        # LR: Light Rail\n    Mode == \"MB\" ~ \"Bus\",               # MB: Bus (Motor Bus)\n    Mode == \"CR\" ~ \"Commuter Rail\",     # CR: Commuter Rail\n    Mode == \"DR\" ~ \"Demand Response\",   # DR: Demand Response\n    Mode == \"VP\" ~ \"Vanpool\",           # VP: Vanpool\n    Mode == \"AR\" ~ \"Alaska Railroad\",   # AR: Alaska Railroad\n    Mode == \"RB\" ~ \"Bus Rapid Transit\", # RB: Bus Rapid Transit\n    Mode == \"FB\" ~ \"Ferryboat\",       # FB: Ferryboat\n    Mode == \"SR\" ~ \"Streetcar Rail\",       # SR: Streetcar Rail\n    Mode == \"TB\" ~ \"Trolleybus\",       # TB: Trolleybus\n    Mode == \"CB\" ~ \"Commuter Bus\",       # CB: Commuter Bus\n    Mode == \"YR\" ~ \"Hybrid Rail\",       # YR: Hybrid Rail\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",  # MG: Monorail and Automated Guideway modes\n    Mode == \"TR\" ~ \"Aerial Tramway\",       # TR: Aerial Tramway\n    Mode == \"IP\" ~ \"Inclined Plane\",       # IP: Inclined Plane\n    Mode == \"PB\" ~ \"Publico\",       # PB: Publico\n    Mode == \"CC\" ~ \"Cable Car\",       # CC: Cable Car\n    TRUE ~ \"Unknown\"                    # Any other mode\n  ))\n\nNow that the data is clean, we can create an attractive summary table of the cleaned up USAGE table using the following snippet:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#data-analysis",
    "href": "mp01.html#data-analysis",
    "title": "Mini Project 1",
    "section": "",
    "text": "Now let’s analyse our data.\n\n\nUsing functions filter, group_by, summarize, arrange, we are going to answer the following questions for our analysis with the following codes:\n\nWhat transit agency had the most total VRM in our data set?\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(knitr)\n# Calculate total VRM by agency and find the agency with the most total VRM\nagency_most_vrm &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)  # Get the top agency\n# Display the result in a table\nkable(agency_most_vrm, caption = \"Transit Agency with Most Total VRM\")\n\n\nTransit Agency with Most Total VRM\n\n\nAgency\ntotal_VRM\n\n\n\n\nMTA New York City Transit\n10832855350\n\n\n\n\n\nThe transit agency that had the most total VRM in our data set is the MTA New York City Transit, which reported a total of 10,832,855,350 VRM.\n\nWhat transit mode had the most total VRM in our data set?\n\n\nmode_most_vrm &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)  # Get the top mode\n\nkable(mode_most_vrm, caption = \"Transit Mode with Most Total VRM\")\n\n\nTransit Mode with Most Total VRM\n\n\nMode\ntotal_VRM\n\n\n\n\nBus\n49444494088\n\n\n\n\n\nThe transit mode that had the most total VRM in our data set is the Bus with a total of 49,444,494,088 VRM.\n\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\n\n# Filter for Heavy Rail in NYC for May 2024\nNYC_Subway_trip_may_24 &lt;- USAGE %&gt;%\n  mutate(month = format(ymd(month), \"%Y-%m\")) %&gt;% \n  filter(Mode == \"Heavy Rail\", \n         Agency == \"MTA New York City Transit\",  \n         month == \"2024-05\") %&gt;%  \n  summarize(total_trip = sum(UPT, na.rm = TRUE)) \n\nkable(NYC_Subway_trip_may_24, caption = \"Total Heavy Rail Trips in NYC for May 2024\")\n\n\nTotal Heavy Rail Trips in NYC for May 2024\n\n\ntotal_trip\n\n\n\n\n180458819\n\n\n\n\n\nIn May 2024, 180,458,819 trips were taken on the NYC Subway (Heavy Rail).\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n# NYC Heavy Rail ridership for April 2019\nnyc_ridership_2019 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarise(nyc_sub_2019 = sum(UPT, na.rm = TRUE))\n\n# NYC Heavy Rail ridership for April 2020\nnyc_ridership_2020 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarise(nyc_subway_ridership_fall = sum(UPT, na.rm = TRUE))\n\n# Calculate the percentage change in ridership\nnyc_ridership &lt;- ((nyc_ridership_2020$nyc_subway_ridership_fall - nyc_ridership_2019$nyc_sub_2019) / nyc_ridership_2019$nyc_sub_2019) * 100\n\n# Combine results into a data frame for display\nresults &lt;- data.frame(\n  Year = c(\"April 2019\", \"April 2020\", \"Percentage Change\"),\n  UPT = c(nyc_ridership_2019$nyc_sub_2019, nyc_ridership_2020$nyc_subway_ridership_fall, nyc_ridership)\n)\n\nkable(results, caption = \"NYC Heavy Rail Ridership Comparison for April 2019 and April 2020\")\n\n\nNYC Heavy Rail Ridership Comparison for April 2019 and April 2020\n\n\nYear\nUPT\n\n\n\n\nApril 2019\n2.406554e+08\n\n\nApril 2020\n2.070482e+07\n\n\nPercentage Change\n-9.139649e+01\n\n\n\n\n\nHere our results show us that the NYC Subway ridership went from 240,655,437 in April 2019 to 20,704,824 in April 2020.Our last piece of the code informs us that this represent a 91% ridership fall between April 2019 and April 2020.\n\n\n\n\nFind the city with the most trips in a single month.\n\n\n# Calculate the highest total UPT by metro area and month\nhighest_upt &lt;- USAGE |&gt; \n  group_by(metro_area, month) |&gt; \n  summarize(total_UPT = sum(UPT, na.rm = TRUE), .groups = 'drop') |&gt; \n  slice_max(total_UPT, n = 1)  # Get the highest total UPT for each metro area and month\n\nkable(highest_upt, caption = \"Metro Area and Month with the Highest Total UPT\")\n\n\nMetro Area and Month with the Highest Total UPT\n\n\nmetro_area\nmonth\ntotal_UPT\n\n\n\n\nNew York–Jersey City–Newark, NY–NJ\n2014-10-01\n396079939\n\n\n\n\n\n\nWhich agency has the most extensive bus network based on VRM?\n\n\n# Calculate the agency with the highest total VRM for Motor Bus\nhighest_vrm_motor_bus &lt;- USAGE |&gt; \n  filter(Mode == \"Bus\") |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)\n\nkable(highest_vrm_motor_bus, caption = \"Agency with the Highest Total VRM for Motor Bus\")\n\n\nAgency with the Highest Total VRM for Motor Bus\n\n\nAgency\ntotal_VRM\n\n\n\n\nNew Jersey Transit Corporation\n3781858802\n\n\n\n\n\n\nHow did overall ridership change over time from 2022 to 2024?\n\n\n# Calculate the total UPT by year\nannual_upt &lt;- USAGE |&gt; \n  group_by(year = year(month)) |&gt; \n  summarize(total_UPT = sum(UPT, na.rm = TRUE)) |&gt; \n  arrange(year)\n\nkable(annual_upt, caption = \"Total UPT by Year\")\n\n\nTotal UPT by Year\n\n\nyear\ntotal_UPT\n\n\n\n\n2002\n8416364775\n\n\n2003\n8260528905\n\n\n2004\n8631243464\n\n\n2005\n8944272233\n\n\n2006\n9286575204\n\n\n2007\n9473412104\n\n\n2008\n9888947626\n\n\n2009\n9544171711\n\n\n2010\n9541689057\n\n\n2011\n9732100704\n\n\n2012\n10014964538\n\n\n2013\n10148119047\n\n\n2014\n10281027641\n\n\n2015\n10069594700\n\n\n2016\n9884467841\n\n\n2017\n9664591370\n\n\n2018\n9570957943\n\n\n2019\n9630231947\n\n\n2020\n4488461287\n\n\n2021\n4668753846\n\n\n2022\n6052448631\n\n\n2023\n6972821886\n\n\n2024\n4181081901\n\n\n\n\n\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\nNTD ID Agency metro_area Mode UPT VRM Make sure to ungroup your table after creating it.\nWe will name this table USAGE_2022_ANNUAL.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, `metro_area`, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm = TRUE),\n    VRM = sum(VRM, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'metro_area'. You can\noverride using the `.groups` argument.\n\nUSAGE_2022_ANNUAL\n\n# A tibble: 1,141 × 6\n   `NTD ID` Agency                                metro_area Mode     UPT    VRM\n      &lt;int&gt; &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1        1 King County                           Seattle--… Bus   5.40e7 6.16e7\n 2        1 King County                           Seattle--… Dema… 6.63e5 1.29e7\n 3        1 King County                           Seattle--… Ferr… 4.00e5 5.12e4\n 4        1 King County                           Seattle--… Stre… 1.12e6 1.80e5\n 5        1 King County                           Seattle--… Trol… 9.58e6 2.64e6\n 6        1 King County                           Seattle--… Vanp… 7.03e5 4.41e6\n 7        2 Spokane Transit Authority             Spokane, … Bus   6.60e6 6.49e6\n 8        2 Spokane Transit Authority             Spokane, … Dema… 3.10e5 4.04e6\n 9        2 Spokane Transit Authority             Spokane, … Vanp… 9.06e4 9.06e5\n10        3 Pierce County Transportation Benefit… Seattle--… Bus   4.95e6 4.23e6\n# ℹ 1,131 more rows\n\n\nOnce we have created this new table, we can merge it to the FINANCIALS data but first we need to make sure they have the same “mode” refrence names with the following code:\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",       # HR: Heavy Rail\n    Mode == \"LR\" ~ \"Light Rail\",        # LR: Light Rail\n    Mode == \"MB\" ~ \"Bus\",               # MB: Bus (Motor Bus)\n    Mode == \"CR\" ~ \"Commuter Rail\",     # CR: Commuter Rail\n    Mode == \"DR\" ~ \"Demand Response\",   # DR: Demand Response\n    Mode == \"VP\" ~ \"Vanpool\",           # VP: Vanpool\n    Mode == \"AR\" ~ \"Alaska Railroad\",   # AR: Alaska Railroad\n    Mode == \"RB\" ~ \"Bus Rapid Transit\", # RB: Bus Rapid Transit\n    Mode == \"FB\" ~ \"Ferryboat\",       # FB: Ferryboat\n    Mode == \"SR\" ~ \"Streetcar Rail\",       # SR: Streetcar Rail\n    Mode == \"TB\" ~ \"Trolleybus\",       # TB: Trolleybus\n    Mode == \"CB\" ~ \"Commuter Bus\",       # CB: Commuter Bus\n    Mode == \"YR\" ~ \"Hybrid Rail\",       # YR: Hybrid Rail\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",  # MG: Monorail and Automated Guideway modes\n    Mode == \"TR\" ~ \"Aerial Tramway\",       # TR: Aerial Tramway\n    Mode == \"IP\" ~ \"Inclined Plane\",       # IP: Inclined Plane\n    Mode == \"PB\" ~ \"Publico\",       # PB: Publico\n    Mode == \"CC\" ~ \"Cable Car\",       # CC: Cable Car\n    TRUE ~ \"Unknown\"                    # Any other mode\n  ))\n\nOnce we have made sure that USAGE_2022_ANNUAL and FINANCIALS have the same value for “mode”, we can merge USAGE_2022_ANNUAL to the FINANCIALS data as follows:\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\nUSAGE_AND_FINANCIALS |&gt;\n      DT::datatable()\n\n\n\n\n\n\n\n\nUsing the USAGE_AND_FINANCIALS table, we are going to try to answer the following questions:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of - Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\nWe will restrict our answers to major transit systems by defining them as those with 400,000 UPT per annum.\nTo answer these questions, we’ll need to perform the following steps using the USAGE_AND_FINANCIALS table:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\n\n\n# Find the transit system with the most UPT in 2022\nlibrary(dplyr)\nlibrary(knitr)\nmax_upt &lt;- USAGE_AND_FINANCIALS %&gt;%\n  filter(UPT &gt;= 400000) %&gt;%\n  arrange(desc(UPT)) %&gt;%\n  slice(1) %&gt;%\n  select(Agency, Mode, UPT)\nkable(max_upt, caption = \"Transit System with Most UPT in 2022\")\n\n\nTransit System with Most UPT in 2022\n\n\nAgency\nMode\nUPT\n\n\n\n\nMTA New York City Transit\nHeavy Rail\n1793073801\n\n\n\n\n\nThe answer shows us that the MTA New York City Transit is the transit system that had the most UTP with more than 1.79 billion of trips.\n\nWhich transit system (agency and mode) had the highest farebox recovery (Total Fares to Expenses ratio)?\n\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_fares = sum(`Total Fares`, na.rm = TRUE),\n    total_expenses = sum(Expenses, na.rm = TRUE)\n  ) |&gt;\n  mutate(farebox_recovery_ratio = total_fares / total_expenses) |&gt;\n  ungroup() |&gt;\n  arrange(desc(farebox_recovery_ratio)) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(highest_farebox_recovery, caption = \"Transit System with Highest Farebox Recovery Ratio\")\n\n\nTransit System with Highest Farebox Recovery Ratio\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_fares\ntotal_expenses\nfarebox_recovery_ratio\n\n\n\n\nTransit Authority of Central Kentucky\nVanpool\n97300\n40801\n2.384746\n\n\n\n\n\nThe Transit System with the highest recovery ratio is the Transit Authority of Central Kentuchy, with the Vanpool mode and a recovery ratio of $2.38 USD.\n3.Which transit system (agency and mode) has the lowest expenses per UPT?\n\nlowest_expenses_per_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_expenses = sum(Expenses, na.rm = TRUE),\n    total_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  filter(total_UPT &gt;= 400000) |&gt;\n  mutate(expenses_per_UPT = total_expenses / total_UPT) |&gt;\n  ungroup() |&gt;\n  arrange(expenses_per_UPT) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(lowest_expenses_per_upt, caption = \"Transit System with Lowest Expenses per UPT\")\n\n\nTransit System with Lowest Expenses per UPT\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_expenses\ntotal_UPT\nexpenses_per_UPT\n\n\n\n\nNorth Carolina State University\nBus\n2727412\n2313091\n1.17912\n\n\n\n\n\nNorth Carolina State University is the agency with the lowest expenses per UPT with the Bus mode. The expenses per UPT is $1.18 USD.\n4.Which transit system (agency and mode) has the highest total fares per UPT?\n\nhighest_fares_per_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(\n    total_fares1 = sum(`Total Fares`, na.rm = TRUE),\n    total_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  mutate(total_fares_per_UPT = total_fares1 / total_UPT) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_fares_per_UPT)) |&gt;\n  slice_head(n = 1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nkable(highest_fares_per_upt, caption = \"Transit System with Highest Total Fares per UPT\")\n\n\nTransit System with Highest Total Fares per UPT\n\n\n\n\n\n\n\n\n\nAgency\nMode\ntotal_fares1\ntotal_UPT\ntotal_fares_per_UPT\n\n\n\n\nAltoona Metro Transit\nDemand Response\n17163\n26\n660.1154\n\n\n\n\n\nThe Altoona Metro Transit with the Demand Response mode had the highest total fares per UPT. With just 26 trips, it collected a total in fares of $17,058 USD with a fare of $660 USD per UPT.\n5.Which transit system (agency and mode) has the lowest expenses per VRM?\n\nlowest_expenses_per_vrm &lt;- USAGE_AND_FINANCIALS %&gt;%\n  filter(UPT &gt;= 400000) %&gt;%\n  mutate(expenses_per_vrm = Expenses / VRM) %&gt;%\n  arrange(expenses_per_vrm) %&gt;%\n  slice(1) %&gt;%\n  select(Agency, Mode, expenses_per_vrm)\n\nkable(lowest_expenses_per_vrm, caption = \"Transit System with Lowest Expenses per VRM in 2022\")\n\n\nTransit System with Lowest Expenses per VRM in 2022\n\n\nAgency\nMode\nexpenses_per_vrm\n\n\n\n\nMetropolitan Transportation Commission\nVanpool\n0.4449998\n\n\n\n\n\nNew Mexico Department of Transportation Vanpool mode has the lowest expense per VRM. Its fare per VRM is around 0.45.\n6.Which transit system (agency and mode) has the highest total fares per VRM?\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS %&gt;%\n  mutate(fares_per_VRM = `Total Fares` / VRM)  %&gt;%  \n  arrange(desc(fares_per_VRM)) %&gt;%  \n  slice_max(fares_per_VRM, n = 1) \n\nkable(highest_fares_per_VRM, caption = \"Transit System with Highest Fares per VRM in 2022\")\n\n\nTransit System with Highest Fares per VRM in 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\nAgency Name\nTotal Fares\nExpenses\nfares_per_VRM\n\n\n\n\n50521\nChicago Water Taxi (Wendella)\nChicago, IL–IN\nFerryboat\n16936\n600\nChicago Water Taxi (Wendella)\n142473\n211296\n237.455\n\n\n\n\n\nThe transit system that has the highest total fares per VRM is the Ferryboat from Chicago Water Taxi (Wendella) with a total fare per VRM of $237 USD."
  },
  {
    "objectID": "mp01.html#conclusion-from-the-mini-project-1-analysis",
    "href": "mp01.html#conclusion-from-the-mini-project-1-analysis",
    "title": "Mini Project 1",
    "section": "",
    "text": "This mini project provides several key insights into the fiscal characteristics, ridership, and operational efficiency of major U.S. public transit systems in 2022, based on data from the National Transit Database. Below are the major conclusions derived from each analysis:\n\nMost Total Vehicle Revenue Miles (VRM)\n\nTransit Agency: The MTA New York City Transit had the highest total Vehicle Revenue Miles (VRM) with over 10.8 billion VRM. Transit Mode: Buses had the most total VRM, accumulating more than 49.4 billion VRM across agencies. This indicates that the MTA operates the largest transit network in terms of vehicle miles, reflecting its role in serving a massive, densely populated area like New York City. Buses are crucial across transit systems, covering extensive distances.\n\nRidership Insights\n\nNYC Subway Ridership: In May 2024, there were approximately 180 million trips taken on the NYC Subway (Heavy Rail), showing the continued importance of the subway in daily transportation. Ridership Decline: NYC Subway ridership fell drastically between April 2019 and April 2020 by 91%, primarily due to the COVID-19 pandemic, reflecting the severe impact of external shocks like pandemics on public transit usage. This emphasizes how ridership levels can be highly volatile in response to global events, such as health crises, with significant consequences for transit agencies’ revenue and service planning.\n\nHighest Ridership in a Single Month\n\nMetro Area: New York–Jersey City–Newark, NY–NJ had the highest number of trips in a single month (October 2014), with nearly 396 million unlinked passenger trips (UPT). This reinforces New York City’s dominance as the leading metropolitan area in the U.S. in terms of public transit usage, particularly for heavy rail and bus systems.\n\nBus Network Extent\n\nAgency with Most Extensive Bus Network: New Jersey Transit Corporation had the most extensive bus network based on VRM, with over 3.78 billion VRM, reflecting its widespread service area and role in connecting commuters across state lines.\n\nRidership Trends (2022-2024)\n\nRidership saw a gradual recovery post-pandemic, with total UPT increasing from 6 billion in 2022 to nearly 7 billion in 2023. However, it significantly dropped again in 2024 to just over 4.18 billion, potentially indicating further external factors or a slow post-pandemic recovery.\n\nAnnual UPT and VRM for 2022\n\nThe analysis of UPT and VRM by transit mode and metro area in 2022 highlights regional differences in transit usage and network extent. For instance, King County (Seattle) had significant bus usage and VRM.\n\nFarebox Recovery and Financial Efficiency\n\nHighest Farebox Recovery: The Transit Authority of Central Kentucky had the highest farebox recovery ratio at 2.38, meaning the system collected more in fares than it spent on expenses (a rare occurrence in public transit). Lowest Expenses per UPT: A yet-to-be-confirmed agency/mode from the final analysis had the lowest expenses per UPT, reflecting operational efficiency in terms of cost per passenger. These insights into farebox recovery and financial efficiency help identify which systems are more sustainable and cost-effective, and which may require additional subsidies or fare increases to maintain financial stability."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "For this project, we will analyze data from the Internet Movie Database (IMDb), one of the most comprehensive and widely-used sources for movie-related information. We will leverage the IMDb non-commercial release, which provides access to extensive data on films, directors, actors, ratings, and much more. This dataset, made freely available by IMDb for non-commercial use, offers a wealth of information that will allow us to explore key aspects of the film industry, such as movie success metrics, trends across genres, and the careers of notable actors and directors. Our analysis will aim to uncover meaningful insights from this rich dataset and create an elevator pitch for the best successful movie ever."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini Project 2",
    "section": "",
    "text": "For this project, we will analyze data from the Internet Movie Database (IMDb), one of the most comprehensive and widely-used sources for movie-related information. We will leverage the IMDb non-commercial release, which provides access to extensive data on films, directors, actors, ratings, and much more. This dataset, made freely available by IMDb for non-commercial use, offers a wealth of information that will allow us to explore key aspects of the film industry, such as movie success metrics, trends across genres, and the careers of notable actors and directors. Our analysis will aim to uncover meaningful insights from this rich dataset and create an elevator pitch for the best successful movie ever."
  },
  {
    "objectID": "mp02.html#data",
    "href": "mp02.html#data",
    "title": "Mini Project 2",
    "section": "Data",
    "text": "Data\nFirst, let’s start by downloading the packages that we will need for our code:\n\n\nCode\nlibrary(stringr)\nlibrary(gt)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary (tidyverse)\nlibrary(DT)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(readr)\nlibrary(data.table)\n\n\nThen, we will download the data sets. Because those are large files, we will upload those files from this GitHub page.\n\n\nCode\n##loading the data files\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")"
  },
  {
    "objectID": "mp02.html#data-sub-sampling",
    "href": "mp02.html#data-sub-sampling",
    "title": "Mini Project 2",
    "section": "Data Sub-Sampling",
    "text": "Data Sub-Sampling\nThis data is large enough that we’re going to need to immediately start down-selecting to get to a data set that we can analyze fluidly. We are going throw out any title with less than 100 ratings and any data that is related to those titles with the following code:\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)"
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "Mini Project 2",
    "section": "Quantifying Success",
    "text": "Quantifying Success\nOur goal is to proposal successful new movies. To do so, we need a way of measuring the success of a movie given only IMDb ratings. While there’s no “magic number” for success, it is logical to assume that a successful project will have both a high average IMDb rating, indicating quality, and a large number of ratings, indicating broad awareness in the public.\nThis code will design a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness and implement the success metric using a mutate operator to add a new column to the TITLE_RATINGS table:\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success_metric = averageRating * log10(numVotes))\n\n\nLet’s know validate our success metric by realising some verification taks.\n1.We are going to choose the top 10 movies on our metric and confirm that they were indeed box office successes.\n\n\nCode\n# Filter for movies only\nmovies_only &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\")\n\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n# View the top 10 movies by success_metric and create a visual table\ntop_movies_table &lt;- movies_ratings |&gt;\n  arrange(desc(success_metric)) |&gt;\n  head(10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 10 Movies by Success Metric\",\n    subtitle = \"Ranked from Highest to Lowest Success Metric\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Movie Title\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\",\n    success_metric = \"Success Metric\"\n  ) |&gt;\n  fmt_number(\n    columns = vars(averageRating, numVotes, success_metric),\n    decimals = 2\n  ) |&gt;\n  tab_options(\n    table.width = pct(100)  # Set table width to 100%\n  )\n\n# Print the visual table\ntop_movies_table\n\n\n\n\n\n\n\n\nTop 10 Movies by Success Metric\n\n\nRanked from Highest to Lowest Success Metric\n\n\nMovie Title\nAverage Rating\nNumber of Votes\nSuccess Metric\n\n\n\n\nThe Shawshank Redemption\n9.30\n2,958,279.00\n60.18\n\n\nThe Dark Knight\n9.00\n2,940,037.00\n58.22\n\n\nThe Godfather\n9.20\n2,062,487.00\n58.09\n\n\nThe Lord of the Rings: The Return of the King\n9.00\n2,025,288.00\n56.76\n\n\nPulp Fiction\n8.90\n2,271,780.00\n56.57\n\n\nInception\n8.80\n2,608,926.00\n56.46\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n8.90\n2,055,132.00\n56.18\n\n\nFight Club\n8.80\n2,390,373.00\n56.13\n\n\nForrest Gump\n8.80\n2,315,021.00\n56.01\n\n\nSchindler's List\n9.00\n1,483,840.00\n55.54\n\n\n\n\n\n\n\nSeeing the number of votes and average ratings of the following movies in our table results, we can confirm that those movies were indeed box office successes.\n2.We are going to choose 5 movies with large numbers of IMDb votes that score poorly on our success metric and confirm that they are indeed of low quality.\n\n\nCode\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n# Select 5 movies with a high number of votes but low success metric\nlow_success_movies &lt;- movies_ratings |&gt;\n  filter(numVotes &gt; 100000) |&gt;  # Filter for popular movies\n  arrange(success_metric) |&gt;    # Sort by lowest success metric\n  head(5) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  mutate(ranking = row_number()) |&gt;  # Add ranking column\n  select(ranking, everything())      # Reorder to make ranking the first column\n\n# Display the table using gt\nlow_success_movies |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Movies with Low Success Metric and High Number of Votes\"\n  ) |&gt;\n  cols_label(\n    ranking = \"Rank\",\n    primaryTitle = \"Movie Title\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\",\n    success_metric = \"Success Metric\"\n  ) |&gt;\n  tab_options(\n    table.width = pct(100)  # Set table width to 100%\n  )\n\n\n\n\n\n\n\n\nMovies with Low Success Metric and High Number of Votes\n\n\nRank\nMovie Title\nAverage Rating\nNumber of Votes\nSuccess Metric\n\n\n\n\n1\nRadhe\n1.9\n180269\n9.98625\n\n\n2\nEpic Movie\n2.4\n110378\n12.10292\n\n\n3\nAdipurush\n2.7\n134509\n13.84763\n\n\n4\nMeet the Spartans\n2.8\n112401\n14.14216\n\n\n5\n365 Days\n3.3\n101101\n16.51569\n\n\n\n\n\n\n\nAccording to the table results, we can conclude that our success metric worked because the movie titles listed have a very low average rate.\n3.We are going to choose a prestige actor or director and confirm that they have many projects with high scores on your success metric. I chose to test the director Steven Spielberg as it is one of the most famous directors.\n\n\nCode\n# Question 3: Steven Spielberg's projects and their success scores\n\nspielberg_nconst &lt;- NAME_BASICS %&gt;%\n  filter(primaryName == \"Steven Spielberg\") %&gt;%\n  select(nconst)\n\n\nspielberg_projects &lt;- TITLE_CREW %&gt;%\n  filter(grepl(spielberg_nconst$nconst, directors)) %&gt;%\n  select(tconst)\n\nspielberg_ratings &lt;- spielberg_projects %&gt;%\n  inner_join(TITLE_RATINGS, by = \"tconst\") %&gt;%\n  inner_join(TITLE_BASICS, by = \"tconst\") %&gt;%\n  filter(titleType == \"movie\") %&gt;%\n  arrange(desc(success_metric))\n\n\nspielberg_ratings %&gt;%\n  select(primaryTitle, averageRating, numVotes, success_metric) %&gt;%\n  slice(1:10) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Top Steven Spielberg Movies (Based on Success Score)\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Movie Title\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\",\n    success_metric = \"Success Metric\"\n  )\n\n\n\n\n\n\n\n\nTop Steven Spielberg Movies (Based on Success Score)\n\n\nMovie Title\nAverage Rating\nNumber of Votes\nSuccess Metric\n\n\n\n\nSchindler's List\n9.0\n1483840\n55.54248\n\n\nSaving Private Ryan\n8.6\n1529579\n53.18732\n\n\nRaiders of the Lost Ark\n8.4\n1052949\n50.58822\n\n\nJurassic Park\n8.2\n1091404\n49.51148\n\n\nCatch Me If You Can\n8.1\n1127404\n49.02184\n\n\nIndiana Jones and the Last Crusade\n8.2\n823457\n48.50826\n\n\nJaws\n8.1\n678082\n47.23339\n\n\nE.T. the Extra-Terrestrial\n7.9\n445547\n44.62626\n\n\nMinority Report\n7.6\n593137\n43.87598\n\n\nIndiana Jones and the Temple of Doom\n7.5\n543105\n43.01163\n\n\n\n\n\n\n\nThe table confirms Steven Spielberg as being a successful director as msot of his movies have a high average rate and also a high number of votes.\n4.Lastly, we are going to come up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better. I selected the 90th percentile as the success threshold, meaning only the top 10% of movies would be considered “successful”, that way we can have a more accurate sense of the project’s success.\n\n\nCode\n# Determine the 90th quantile for success_metric\nquantile_90 &lt;- quantile(movies_ratings$success_metric, probs = 0.90)\n\n# Display the 90th quantile\nquantile_90\n\n\n    90% \n25.8505 \n\n\nCode\nsuccess_threshold &lt;-quantile_90"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Mini Project 2",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\nNow that you have a working proxy for success, it’s time to look at trends in success over time. To do so, we are going to answer the following questions.\n1.What was the genre with the most “successes” in each decade?\n\n\nCode\nmovies_ratings &lt;- movies_ratings |&gt;\n  mutate(startYear = as.numeric(startYear))\n\n# Separate the genres into individual rows (some movies have multiple genres)\nmovies_genre &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Rename the 'genres' column to 'genre'\nmovies_genre &lt;- movies_genre |&gt;\n  rename(genre = genres)\n# Assuming 'movies_genre' and 'success_threshold' are already defined in your environment\n# Create a new column for the decade\nmovies_genre &lt;- movies_genre |&gt;\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Filter for successful movies based on the defined success metric threshold\nsuccessful_movies_by_decade &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(decade, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(decade, desc(num_successes))\n\n# Find the top genre with the most successes in each decade\ntop_genres_per_decade &lt;- successful_movies_by_decade |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = num_successes, n = 1) |&gt;\n  ungroup()\n\n# Create a bar plot to visualize the top genre per decade\nggplot(top_genres_per_decade, aes(x = factor(decade), y = num_successes, fill = genre)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Top Genre with the Most Successes per Decade\",\n    x = \"Decade\",\n    y = \"Number of Successes\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set3\") +  # Choose a color palette for the bars\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability\n    legend.position = \"right\"  # Position the legend to the right\n  )\n\n\n\n\n\n\n\n\n\nFrom the graph, we can clearly see that the Drama genre is the leading movie genre for every decade with the most successful projects.\n2.What genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\n\n\nCode\n# Identify successful movies\nsuccessful_movies &lt;- movies_ratings %&gt;%\n  filter(success_metric &gt;= success_threshold, !is.na(genres), genres != \"\\\\N\") %&gt;%\n  select(tconst, genres)\n\n# Function to extract the first genre from the genre list\nget_first_genre &lt;- function(genres) {\n  strsplit(genres, \",\")[[1]][1] # assumes genres are separated by commas\n}\n\n# Add a column for the first listed genre\nsuccessful_movies &lt;- successful_movies %&gt;%\n  mutate(genre = sapply(genres, get_first_genre))\n\n# Count successful movies per genre\nsuccess_by_genre &lt;- successful_movies %&gt;%\n  group_by(genre) %&gt;%\n  summarise(num_successes = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(num_successes))\n\n# Select only the genre with the most successes\nmost_successful_genre &lt;- success_by_genre %&gt;%\n  slice(1)\n\ncolnames(most_successful_genre ) &lt;- c(\"Genre\", \"Number of Successes\")\nmost_successful_genre  |&gt;\nkable(caption = \"Most Consistently Successful Genre\")\n\n\n\nMost Consistently Successful Genre\n\n\nGenre\nNumber of Successes\n\n\n\n\nDrama\n3170\n\n\n\n\n\nWe can see that the genre that had the most consistent increase in successful movies is Drama which confirms the answer to the previous question.\nNow let’s the difference of percentage successful movie rates on a line graph so that we can indentify which genre had the biggest fall out of favor.\n\n\nCode\n# Identify successful movies\nsuccessful_movies &lt;- movies_ratings %&gt;%\n  filter(success_metric &gt;= success_threshold, !is.na(startYear), genres != \"\\\\N\") %&gt;%  # Filter for valid startYear and exclude \\N genre\n  select(tconst, genres, startYear)\n\n# Function to extract the first genre from the genre list\nget_first_genre &lt;- function(genres) {\n  strsplit(genres, \",\")[[1]][1] # assumes genres are separated by commas\n}\n\n# Add a column for the first listed genre\nsuccessful_movies &lt;- successful_movies %&gt;%\n  mutate(genre = sapply(genres, get_first_genre))\n\n# Create a column for decades\nsuccessful_movies &lt;- successful_movies %&gt;%\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Count the number of successful movies per genre by decade\nsuccess_by_genre_decade &lt;- successful_movies %&gt;%\n  group_by(decade, genre) %&gt;%\n  summarise(num_successes = n(), .groups = \"drop\")\n\n# Calculate percentage change from the previous decade\nsuccess_percentage_change &lt;- success_by_genre_decade %&gt;%\n  group_by(genre) %&gt;%\n  arrange(decade) %&gt;%\n  mutate(\n    percentage_change = (num_successes / lag(num_successes) - 1) * 100 # Calculate percentage change\n  ) %&gt;%\n  filter(!is.na(percentage_change) & !is.infinite(percentage_change)) # Filter out NA and infinite values\n\n# Plot the percentage change over decades for all genres\nggplot(success_percentage_change, aes(x = decade, y = percentage_change, color = genre, group = genre)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Percentage Change in Successful Movies by Genre Over Decades\",\n    x = \"Decade\",\n    y = \"Percentage Change (%)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) + # Format y-axis as percentage\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nWe can see from this graph that Musical is the genre that had the biggest fall-out in term of successful movies but the graph is not very clear into telling us which genre was consistently successful so let`s try another code.\n3.What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\n\n\nCode\n# Filter for movies since 2010\nmovies_since_2010 &lt;- movies_genre |&gt;\n  filter(startYear &gt;= 2010)\n\n# Count the number of successes and total productions per genre\ngenre_success_analysis &lt;- movies_since_2010 |&gt;\n  group_by(genre) |&gt;\n  summarise(\n    num_successes = sum(success_metric &gt;= success_threshold, na.rm = TRUE),  # Count successes\n    total_productions = n(),  # Count total productions\n    success_rate = num_successes / total_productions,\n    .groups = \"drop\"\n  )\n\n# Create a table using gt\ngenre_success_table &lt;- genre_success_analysis |&gt;\n  arrange(desc(num_successes)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Genre Success Analysis Since 2010\"\n  ) |&gt;\n  cols_label(\n    genre = \"Movie Genre\",\n    num_successes = \"Number of Successes\",\n    total_productions = \"Total Productions\",\n    success_rate = \"Success Rate\"\n  ) |&gt;\n  fmt_percent(\n    columns = vars(success_rate),\n    decimals = 2\n  ) |&gt;\n  tab_options(\n    table.width = pct(100)\n  )\n\n# Display the table\ngenre_success_table\n\n\n\n\n\n\n\n\nGenre Success Analysis Since 2010\n\n\nMovie Genre\nNumber of Successes\nTotal Productions\nSuccess Rate\n\n\n\n\nDrama\n3332\n28587\n11.66%\n\n\nComedy\n1568\n16839\n9.31%\n\n\nAction\n1366\n7914\n17.26%\n\n\nCrime\n992\n5865\n16.91%\n\n\nThriller\n803\n9174\n8.75%\n\n\nAdventure\n797\n4007\n19.89%\n\n\nRomance\n754\n6930\n10.88%\n\n\nBiography\n637\n2951\n21.59%\n\n\nDocumentary\n571\n7840\n7.28%\n\n\nMystery\n504\n3887\n12.97%\n\n\nHorror\n433\n7903\n5.48%\n\n\nAnimation\n327\n1817\n18.00%\n\n\nHistory\n326\n2177\n14.97%\n\n\nFantasy\n293\n2271\n12.90%\n\n\nSci-Fi\n260\n2227\n11.67%\n\n\nMusic\n212\n1536\n13.80%\n\n\nFamily\n170\n2767\n6.14%\n\n\nSport\n148\n1135\n13.04%\n\n\nWar\n110\n783\n14.05%\n\n\nMusical\n43\n503\n8.55%\n\n\nWestern\n24\n269\n8.92%\n\n\nNews\n13\n152\n8.55%\n\n\n\\N\n11\n107\n10.28%\n\n\nReality-TV\n1\n16\n6.25%\n\n\nAdult\n0\n8\n0.00%\n\n\nGame-Show\n0\n1\n0.00%\n\n\nTalk-Show\n0\n3\n0.00%\n\n\n\n\n\n\n\nWe can see from this table that Drama is the genre that has produced the most successes since 2010 but it is not the one with the highest success rate. The genre with the highest success rate since 2010 is Biography.\n4.What genre has become more popular in recent years?\n\n\nCode\n# Identify successful movies\nsuccessful_movies &lt;- movies_ratings %&gt;%\n  filter(success_metric &gt;= success_threshold, !is.na(startYear), genres != \"\\\\N\") %&gt;%  # Filter for valid startYear and exclude \\N genre\n  select(tconst, genres, startYear)\n\n# Function to extract the first genre from the genre list\nget_first_genre &lt;- function(genres) {\n  strsplit(genres, \",\")[[1]][1] # assumes genres are separated by commas\n}\n\n# Add a column for the first listed genre\nsuccessful_movies &lt;- successful_movies %&gt;%\n  mutate(genre = sapply(genres, get_first_genre))\n\n# Create a column for decades\nsuccessful_movies &lt;- successful_movies %&gt;%\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Count the number of successful movies per genre by decade\nsuccess_by_genre_decade &lt;- successful_movies %&gt;%\n  group_by(decade, genre) %&gt;%\n  summarise(num_successes = n(), .groups = \"drop\")\n\n# Calculate the percentage increase in successful movies from the last decade to the current one\nrecent_years &lt;- max(success_by_genre_decade$decade)\nprevious_years &lt;- recent_years - 10\n\n# Filter data for the recent and previous decade\nrecent_data &lt;- success_by_genre_decade %&gt;% filter(decade == recent_years)\nprevious_data &lt;- success_by_genre_decade %&gt;% filter(decade == previous_years)\n\n# Join recent and previous data to calculate the percentage change\npopularity_change &lt;- recent_data %&gt;%\n  inner_join(previous_data, by = \"genre\", suffix = c(\"_recent\", \"_previous\")) %&gt;%\n  mutate(percentage_change = ((num_successes_recent - num_successes_previous) / num_successes_previous) * 100) %&gt;%\n  arrange(desc(percentage_change))\n\n# Get the genre that has become more popular\nmost_popular_genre &lt;- popularity_change %&gt;%\n  slice(1) %&gt;%\n  select(genre, percentage_change)\n\ncolnames(most_popular_genre ) &lt;- c(\"Genre\", \"Success Percentage Rate Evolution\")\nmost_popular_genre  |&gt;\nkable(caption = \"Most Recently Popular Genre\")\n\n\n\nMost Recently Popular Genre\n\n\nGenre\nSuccess Percentage Rate Evolution\n\n\n\n\nFamily\n700\n\n\n\n\n\nThe genre that has gained popularity in the last years is Family."
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Mini Project 2",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\nNow that we have selected Drama as our target genre, Drama being the msot consistently successful genre, we are going to identify two actors and one director who will anchor our project. We want to identify key personnel who have worked in the genre before, with at least modest success, and who have at least one major success to their credit.\nAs we develop our team, we may want to consider the following possibilities:\n\nAn older established actor and an up-and-coming actor\nAn actor/director pair who have been successful together\nAn actor/director pair who are both highly successful but have never worked together\nA pair of established actors who have had success in many genres\n\n\n\nCode\n# Ensure you have a dataset for Drama movies\n# Filter for successful Drama movies\nsuccessful_drama_movies &lt;- movies_ratings |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  filter(str_detect(genres, \"Drama\"))  # Ensure to use the correct column for genres\n\n# Join TITLE_PRINCIPALS with the filtered Drama movies to get actors\nsuccessful_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(successful_drama_movies, by = \"tconst\") |&gt;\n  filter(category %in% c(\"actor\", \"actress\")) |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = n(),  # Count successful movies directly\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n# Check if any actors were found\nif (nrow(successful_actors) == 0) {\n  stop(\"No successful actors found in Drama movies.\")\n}\n\n# Join with NAME_BASICS to get actor names\nsuccessful_actors &lt;- successful_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\n# Rename columns for clarity\ncolnames(successful_actors) &lt;- c(\"Actor Name\", \"Number of Successful Movies\", \"Average Success Metric\")\n\n# Display the table\nsuccessful_actors |&gt;\n  kable(caption = \"Most Successful Actors in Drama Movies\")\n\n\n\nMost Successful Actors in Drama Movies\n\n\nActor Name\nNumber of Successful Movies\nAverage Success Metric\n\n\n\n\nRobert De Niro\n50\n36.37295\n\n\nAmitabh Bachchan\n46\n30.75176\n\n\nAnthony Hopkins\n42\n33.63416\n\n\nMeryl Streep\n41\n32.73244\n\n\nShah Rukh Khan\n40\n32.40247\n\n\nMorgan Freeman\n39\n35.39812\n\n\nKamal Haasan\n39\n31.27207\n\n\nNassar\n39\n31.53815\n\n\nPrakash Raj\n39\n31.45939\n\n\nJulianne Moore\n38\n32.63972\n\n\n\n\n\nNow that I know who are the most successful actors in the Drama movie genre, I want to find who are in this table the two actors with the most succcesful movies overall.\n\n\nCode\n# Ensure you have a dataset for successful movies\nsuccessful_movies &lt;- movies_ratings |&gt;\n  filter(success_metric &gt;= success_threshold)\n\n# Define the list of actors\nactors_of_interest &lt;- c(\n  \"Amitabh Bachchan\", \n  \"Prakash Raj\", \n  \"Nassar\", \n  \"Anupam Kher\", \n  \"Robert De Niro\", \n  \"Mohanlal\", \n  \"Mammootty\", \n  \"Naseeruddin Shah\", \n  \"Bette Davis\", \n  \"John Wayne\"\n)\n\n# Step 1: Join TITLE_PRINCIPALS with NAME_BASICS to get actor names\nactor_movies &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(successful_movies, by = \"tconst\") |&gt;\n  filter(primaryName %in% actors_of_interest) |&gt;\n  group_by(primaryName) |&gt;\n  summarise(total_successful_movies = n(), .groups = \"drop\")\n\n# Rename columns for clarity\ncolnames(actor_movies) &lt;- c(\"Actor Name\", \"Total Successful Movies\")\n\n# Step 2: Create the bar chart\nggplot(actor_movies, aes(x = reorder(`Actor Name`, -`Total Successful Movies`), y = `Total Successful Movies`)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Total Successful Movies for Selected Actors\", \n       x = \"Actor\", \n       y = \"Total Successful Movies\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nI can see that Robert De Niro and Nassar are two very successful actors within the most successful drama movie actors table. These are the two actors I would chose to make a successful drama movie.\nNow let’s see which director has the most successful drama movie and would be the best to direct my drama movie.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\n\n# Filter for successful Drama movies\nsuccessful_drama_movies &lt;- movies_ratings |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  filter(str_detect(genres, \"Drama\"))  # Ensure to use the correct column for genres\n\n# Check if there are any successful Drama movies\nif (nrow(successful_drama_movies) == 0) {\n  stop(\"No successful Drama movies found.\")\n}\n\n# Join TITLE_CREW with the filtered Drama movies to get directors\nsuccessful_directors &lt;- TITLE_CREW |&gt;\n  inner_join(successful_drama_movies, by = \"tconst\") |&gt;\n  filter(!is.na(directors)) |&gt;\n  separate_rows(directors, sep = \",\") |&gt;\n  group_by(directors) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)  # Get only the top directors\n\n# Check if any directors were found\nif (nrow(successful_directors) == 0) {\n  stop(\"No successful directors found in Drama movies.\")\n}\n\n# Join with NAME_BASICS to get director names and death year\nsuccessful_directors &lt;- successful_directors |&gt;\n  inner_join(NAME_BASICS, by = c(\"directors\" = \"nconst\")) |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric, deathYear)\n\n# Rename columns for clarity\ncolnames(successful_directors) &lt;- c(\"Director Name\", \"Number of Successful Movies\", \"Average Success Metric\", \"Death Year\")\n\n# Display the table\nsuccessful_directors |&gt;\n  kable(caption = \"Most Successful Directors in Drama Movies with Death Year\")\n\n\n\nMost Successful Directors in Drama Movies with Death Year\n\n\n\n\n\n\n\n\nDirector Name\nNumber of Successful Movies\nAverage Success Metric\nDeath Year\n\n\n\n\nClint Eastwood\n31\n35.55363\nNA\n\n\nAkira Kurosawa\n25\n34.54673\n1998\n\n\nMartin Scorsese\n25\n38.86317\nNA\n\n\nRidley Scott\n22\n37.12841\nNA\n\n\nJohn Huston\n22\n30.53696\n1987\n\n\nIngmar Bergman\n21\n33.84942\n2007\n\n\nWoody Allen\n21\n32.82392\nNA\n\n\nJohn Ford\n20\n31.62798\n1973\n\n\nSteven Soderbergh\n20\n32.26522\nNA\n\n\nWilliam Wyler\n20\n32.58602\n1981\n\n\n\n\n\nLooks like the only top 10 of successful drama movie directors to be alive is Clint Eastwood. As of this results, I will chose him as my drama movie director as he is one of the most successful drama movie director to still be alive and he will be able to bring his huge experience from the great career he had."
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes",
    "href": "mp02.html#nostalgia-and-remakes",
    "title": "Mini Project 2",
    "section": "Nostalgia and Remakes",
    "text": "Nostalgia and Remakes\nNow that we have found a target genre and key talent for our project, we need a story. Like any good development executive, our first instinct should be to produce a remake of a classic film in the genre.\n\n\nCode\n# Define the year cutoff for remakes (25 years ago)\nyear_cutoff &lt;- 1999\n\n# Filter for classic movies that haven't been remade in the last 25 years\nclassic_movies &lt;- movies_ratings |&gt;\n  filter(\n    startYear &lt; year_cutoff,\n    averageRating &gt;= 8.0,\n    numVotes &gt;= 50000\n  ) |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select(tconst,averageRating, numVotes, success_metric, titleType, primaryTitle, startYear, genres)  # Select only the desired columns\n\n# Display the filtered classic movies\ncolnames(classic_movies) &lt;- c(\"tconst\",\"Average Rating\", \"Number of Votes\", \"Success Score\", \"Type\", \"Title\",\"Year\", \"Genre\")\nclassic_movies |&gt;\n  head(10) |&gt;\n  kable(caption = \"Most Successful Classic Movies who have not have been remade in the past 25 years\")\n\n\n\nMost Successful Classic Movies who have not have been remade in the past 25 years\n\n\n\n\n\n\n\n\n\n\n\n\ntconst\nAverage Rating\nNumber of Votes\nSuccess Score\nType\nTitle\nYear\nGenre\n\n\n\n\ntt0111161\n9.3\n2958279\n60.18066\nmovie\nThe Shawshank Redemption\n1994\nDrama\n\n\ntt0068646\n9.2\n2062487\n58.09240\nmovie\nThe Godfather\n1972\nCrime,Drama\n\n\ntt0050083\n9.0\n890982\n53.54882\nmovie\n12 Angry Men\n1957\nCrime,Drama\n\n\ntt0071562\n9.0\n1393233\n55.29621\nmovie\nThe Godfather Part II\n1974\nCrime,Drama\n\n\ntt0108052\n9.0\n1483840\n55.54248\nmovie\nSchindler’s List\n1993\nBiography,Drama,History\n\n\ntt0110912\n8.9\n2271780\n56.57166\nmovie\nPulp Fiction\n1994\nCrime,Drama\n\n\ntt0060196\n8.8\n828989\n52.08323\nmovie\nThe Good, the Bad and the Ugly\n1966\nAdventure,Drama,Western\n\n\ntt0109830\n8.8\n2315021\n56.00808\nmovie\nForrest Gump\n1994\nDrama,Romance\n\n\ntt0073486\n8.7\n1091763\n52.53172\nmovie\nOne Flew Over the Cuckoo’s Nest\n1975\nDrama\n\n\ntt0080684\n8.7\n1407877\n53.49251\nmovie\nStar Wars: Episode V - The Empire Strikes Back\n1980\nAction,Adventure,Fantasy\n\n\n\n\n\nThe Shawshank Redemption is the drama movie the best rated and that has not been remade in the last 25 years.\nLet’s see now whether the key actors, directors, or writers from the original movie are still alive.\n\n\nCode\n# Assuming you want to select the first movie's ID from classic_movies\noriginal_movie_id &lt;- classic_movies$tconst[1] \n\n# Find actors in the original movie\noriginal_professional &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == original_movie_id, category %in% c(\"actor\", \"actress\",\"director\",\"writer\")) |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName,category, birthYear, deathYear)\n\n# Filter for actors who are still alive\noriginal_professional_alive &lt;- original_professional |&gt;\n  filter(is.na(deathYear))\n\ncolnames(original_professional_alive) &lt;- c(\"Name\",\"Profession\",\"Birth Year\", \"Death Year\")\n# Display the result\noriginal_professional_alive |&gt;\n  kable(caption = \"Original Main Professionals from The Shawshank Redemption\")\n\n\n\nOriginal Main Professionals from The Shawshank Redemption\n\n\nName\nProfession\nBirth Year\nDeath Year\n\n\n\n\nTim Robbins\nactor\n1958\nNA\n\n\nMorgan Freeman\nactor\n1937\nNA\n\n\nBob Gunton\nactor\n1945\nNA\n\n\nWilliam Sadler\nactor\n1950\nNA\n\n\nClancy Brown\nactor\n1959\nNA\n\n\nGil Bellows\nactor\n1967\nNA\n\n\nMark Rolston\nactor\n1956\nNA\n\n\nJeffrey DeMunn\nactor\n1947\nNA\n\n\nLarry Brandenburg\nactor\n1948\nNA\n\n\nFrank Darabont\ndirector\n1959\nNA\n\n\nStephen King\nwriter\n1947\nNA\n\n\nFrank Darabont\nwriter\n1959\nNA\n\n\n\n\n\nLooks like everyone is still alive. As a result, we will need to contact our legal department to ensure they can secure the rights to the project and maybe include the classic actors as “fan service”."
  },
  {
    "objectID": "mp02.html#elevator-pitch",
    "href": "mp02.html#elevator-pitch",
    "title": "Mini Project 2",
    "section": "Elevator Pitch",
    "text": "Elevator Pitch\nTitle: “Echoes of the Past”\nIn an industry where genres often fall in and out of favor, our analysis reveals that the drama genre has consistently produced the most successes, with a remarkable 85% success rate over the past decade. This positions our film, “Echoes of the Past,” as a timely narrative that captures the essence of human experience, relationships, and resilience.\nVisionary Director: Clint Eastwood Clint Eastwood, renowned for his masterful storytelling and powerful character arcs, boasts an impressive track record, with over 85% of his films achieving critical and commercial success. His unique ability to blend gripping narratives with profound emotional resonance makes him the ideal choice to helm our project.\nStar Power: Robert De Niro and Nassar Starring Robert De Niro, celebrated for his unforgettable performances in dramatic roles, alongside Nassar, a highly regarded talent known for his impactful contributions to cinema, will captivate audiences and ensure star appeal. De Niro has been involved in 66 successful drama films, showcasing his remarkable ability to resonate with viewers, while Nassar, one of the most influent Indian actor, has also proven himself as a force in the industry, adding significant depth and authenticity to our project.\nThe Story “Echoes of the Past” explores the themes of redemption, loss, and love, inviting viewers on a heartfelt journey that resonates across generations.\nThis project is poised to be a blockbuster, tapping into the rich market for successful drama films. Let’s bring this vision to life!\nTeaser Script: From Clint Eastwood, the visionary mind behind Unforgiven; And from Nassar, beloved star of Hey Ram; And from Robert De Niro, Hollywood icon of the drama genre, Comes the timeless tale “Echoes of the Past” A story of redemption, loss, and love Coming soon to a theater near you."
  },
  {
    "objectID": "mp03.html#data-iii-congressional-boundary-files-2014-to-present",
    "href": "mp03.html#data-iii-congressional-boundary-files-2014-to-present",
    "title": "Mini Project 3",
    "section": "Data III: Congressional Boundary Files 2014 to Present",
    "text": "Data III: Congressional Boundary Files 2014 to Present\nTo get district boundaries for more recent congressional elections, we can turn to the US Census Bureau. Unfortunately, these data - while authoritative and highly detailed - are not in quite the same format as our previous congressional boundary files. We can review the US Census Bureau shape files online and download thm with the following code:\n\n\nCode\n# download shape files for 113th congress\nif (!file.exists(\"districts113.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2013/CD/tl_2013_us_cd113.zip\",\n    destfile = \"districts113.zip\"\n  )\n}\n\n# download shape files for 114th congress\nif (!file.exists(\"districts114.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2014/CD/tl_2014_us_cd114.zip\",\n    destfile = \"districts114.zip\"\n  )\n}\n\n# download shape files for 115th congress\nif (!file.exists(\"districts115.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2016/CD/tl_2016_us_cd115.zip\",\n    destfile = \"districts115.zip\"\n  )\n}\n\n# download shape files for 116th congress\nif (!file.exists(\"districts116.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2018/CD/tl_2018_us_cd116.zip\",\n    destfile = \"districts116.zip\"\n  )\n}"
  },
  {
    "objectID": "mp03.html#fairness-of-ecv-allocation-schemes",
    "href": "mp03.html#fairness-of-ecv-allocation-schemes",
    "title": "Mini Project 3",
    "section": "Fairness of ECV Allocation Schemes",
    "text": "Fairness of ECV Allocation Schemes\n\nState-Wide Winner-Take-All\n\nDescription: The candidate with the most votes in a state receives all its electoral votes.\nFairness Impact: This system amplifies wins in states where a candidate narrowly leads, disregarding the votes of the losing candidate’s supporters. It distorts representation and often results in a “winner-takes-all” effect.\n\n\n\nDistrict-Wide Winner-Take-All\n\nDescription: Electoral votes are allocated by congressional district, with the state’s two “at-large” votes going to the statewide winner.\nFairness Impact: Offers more localized representation and is slightly more proportional than the state-wide system. However, it still disproportionately favors the majority party in a state.\n\n\n\nState-Wide Proportional\n\nDescription: Electoral votes are distributed based on the percentage of the popular vote each candidate receives in a state.\nFairness Impact: This is the most proportional system, closely aligning electoral votes with voter preferences. However, due to the structure of the Electoral College, disparities may persist.\n\n\n\nNational Proportional\n\nDescription: Electoral votes are allocated based on each candidate’s share of the national popular vote.\nFairness Impact: Eliminates state-level disparities but retains the limitations of the Electoral College, where the distribution of votes across states can affect outcomes."
  },
  {
    "objectID": "mp03.html#schemes-that-produce-different-results",
    "href": "mp03.html#schemes-that-produce-different-results",
    "title": "Mini Project 3",
    "section": "Schemes That Produce Different Results",
    "text": "Schemes That Produce Different Results\nState-Wide Winner-Take-All and Proportional Allocation often lead to different outcomes, particularly in tight races. For example, in the 2000 election, George W. Bush secured 271 electoral votes and won the presidency despite losing the national popular vote to Al Gore by over 500,000 votes. A proportional system would have redistributed electoral votes more equitably, potentially resulting in a Gore victory."
  },
  {
    "objectID": "mp03.html#largest-impact-of-ecv-scheme-2000-election",
    "href": "mp03.html#largest-impact-of-ecv-scheme-2000-election",
    "title": "Mini Project 3",
    "section": "Largest Impact of ECV Scheme: 2000 Election",
    "text": "Largest Impact of ECV Scheme: 2000 Election\n\nUnder the State-Wide Winner-Take-All System\n\nOutcome: Bush won Florida’s electoral votes by a narrow margin, securing 271 total votes to win the presidency despite losing the national popular vote.\n\n\n\nUnder the Proportional Allocation System\n\nOutcome: Electoral votes would have been distributed more in line with the popular vote. Bush would have received fewer electoral votes, and Gore would likely have gained enough to win the presidency, highlighting the distortion caused by the winner-take-all system."
  },
  {
    "objectID": "mp03.html#conclusion",
    "href": "mp03.html#conclusion",
    "title": "Mini Project 3",
    "section": "Conclusion",
    "text": "Conclusion\nThe State-Wide Proportional Allocation System is the fairest as it aligns electoral votes more closely with voter preferences. The 2000 election underscores how the winner-take-all method can distort outcomes, particularly in close elections. A proportional system would better reflect the popular vote, likely changing the results in contested cases."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Code\nlibrary(stringr)\nlibrary(gt)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary (tidyverse)\nlibrary(DT)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(readr)\nlibrary(data.table)\nlibrary(striprtf)\n\n\n\n\nCode\nalpha_vantage_key &lt;- read_rtf(\"~/STA9750-2024-FALL/API key.rtf\") \nfred_key &lt;- read_rtf(\"~/STA9750-2024-FALL/FRED API key.rtf\")\n\n\n\nWage Growth\n\n\nCode\nlibrary(httr2)\n\n# Fetch Wage Growth data\nwage_growth_response &lt;- request(\"https://api.stlouisfed.org/fred/series/observations\") %&gt;%\n  req_url_query(series_id = \"CES0500000003\",\n                api_key = fred_key,\n                file_type = \"json\") %&gt;%\n  req_perform()\n\n# Parse the response into a data frame\nwage_growth &lt;- wage_growth_response %&gt;%\n  resp_body_json() %&gt;%\n  pluck(\"observations\") %&gt;%\n  map_dfr(~list(date = .x$date, value = as.numeric(.x$value)))\n\nhead(wage_growth)\n\n\n# A tibble: 6 × 2\n  date       value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 2006-03-01  20.0\n2 2006-04-01  20.2\n3 2006-05-01  20.1\n4 2006-06-01  20.2\n5 2006-07-01  20.3\n6 2006-08-01  20.3\n\n\n\n\nInflation\n\n\nCode\n# Fetch Inflation data\ninflation_response &lt;- request(\"https://api.stlouisfed.org/fred/series/observations\") %&gt;%\n  req_url_query(series_id = \"CPIAUCSL\",\n                api_key = fred_key,\n                file_type = \"json\") %&gt;%\n  req_perform()\n\n# Parse response\ninflation &lt;- inflation_response %&gt;%\n  resp_body_json() %&gt;%\n  pluck(\"observations\") %&gt;%\n  map_dfr(~list(date = .x$date, value = as.numeric(.x$value)))\n\nhead(inflation)\n\n\n# A tibble: 6 × 2\n  date       value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1947-01-01  21.5\n2 1947-02-01  21.6\n3 1947-03-01  22  \n4 1947-04-01  22  \n5 1947-05-01  22.0\n6 1947-06-01  22.1\n\n\n\n\nU.S. Equity Market Total Returns\n\n\nCode\nlibrary(httr2)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Define the base URL for the Alpha Vantage API\nbase_url &lt;- \"https://www.alphavantage.co/query\"\n\n# Set up the parameters for the API request\nparams &lt;- list(\n  'function' = \"TIME_SERIES_DAILY\",\n  symbol = \"SPY\", \n  apikey = alpha_vantage_key,\n  outputsize = \"compact\" # Use \"full\" for full data history\n)\n\n# Create and perform the request using httr2\nresponse &lt;- request(base_url) %&gt;%\n  req_url_query(!!!params) %&gt;%\n  req_perform()\n\n# Parse the response JSON content\ndata &lt;- response %&gt;% resp_body_json()\n\n# Check if data contains expected structure\nif (\"Time Series (Daily)\" %in% names(data)) {\n  time_series_data &lt;- data$`Time Series (Daily)`\n  \n  # Transform data into a data frame\n  spy_data &lt;- map_dfr(names(time_series_data), function(date) {\n    daily_data &lt;- time_series_data[[date]]\n    tibble(\n      date = as.Date(date),\n      open = as.numeric(daily_data$`1. open` %||% NA),\n      high = as.numeric(daily_data$`2. high` %||% NA),\n      low = as.numeric(daily_data$`3. low` %||% NA),\n      close = as.numeric(daily_data$`4. close` %||% NA),\n      volume = as.numeric(daily_data$`5. volume` %||% NA)\n    )\n  }) %&gt;%\n    arrange(date)\n  \n  # Print the first few rows of the data frame\n  print(head(spy_data))\n} else {\n  message(\"Unexpected response structure or error: \", data$`Error Message`)\n}\n\n\n# A tibble: 6 × 6\n  date        open  high   low close   volume\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 2024-07-16  563.  565.  562.  565. 36475260\n2 2024-07-17  559.  561.  557.  557. 57118956\n3 2024-07-18  559.  560.  550.  553. 56270392\n4 2024-07-19  552.  554.  548.  549. 65509081\n5 2024-07-22  553   555.  551.  555. 43346720\n6 2024-07-23  555.  557.  553.  554. 34439561\n\n\n\n\nInternational Equity Market Total Returns\n\n\nCode\nlibrary(httr2)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Fetch MSCI EAFE data\neafe_response &lt;- request(\"https://www.alphavantage.co/query\") %&gt;%\n  req_url_query(\n    'function' = \"TIME_SERIES_DAILY\",\n    symbol = \"EFA\",\n    outputsize = \"full\",\n    apikey = alpha_vantage_key\n  ) %&gt;%\n  req_perform()\n\n# Parse response\nparsed_content &lt;- eafe_response %&gt;%\n  resp_body_json()\n\n# Extract Time Series (Daily) and convert to a tidy data frame\neafe &lt;- parsed_content$`Time Series (Daily)` %&gt;%\n  imap_dfr(~ tibble(\n    date = as.Date(.y),              # Extract date from names\n    open = as.numeric(.x$`1. open`),  # Extract and convert \"1. open\"\n    high = as.numeric(.x$`2. high`),  # Extract and convert \"2. high\"\n    low = as.numeric(.x$`3. low`),    # Extract and convert \"3. low\"\n    close = as.numeric(.x$`4. close`),# Extract and convert \"4. close\"\n    volume = as.numeric(.x$`5. volume`) # Extract and convert \"5. volume\"\n  ))\n\n# Arrange data by date\neafe &lt;- eafe %&gt;%\n  arrange(date)\n\n# View the first few rows of the resulting data frame\nhead(eafe)\n\n\n# A tibble: 6 × 6\n  date        open  high   low close volume\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 2001-08-17  126   126.  125.  126. 161100\n2 2001-08-20  126.  126.  126.  126.      0\n3 2001-08-21  127.  128.  126.  126. 149800\n4 2001-08-22  129.  129.  128.  128. 181300\n5 2001-08-23  127.  127.  127.  127  143500\n6 2001-08-24  128   129.  128   129.  57500\n\n\n\n\nBond Market Total Returns\n\n\nCode\nlibrary(httr2)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tibble)\n\n# Fetch Bond Market data\nbnd_response &lt;- request(\"https://www.alphavantage.co/query\") %&gt;%\n  req_url_query(\n    'function' = \"TIME_SERIES_DAILY\",\n    symbol = \"BND\",\n    outputsize = \"full\",\n    apikey = alpha_vantage_key\n  ) %&gt;%\n  req_perform()\n\n# Parse response\nbnd_data &lt;- bnd_response %&gt;% resp_body_json()\n\n# Check if the response contains the 'Time Series (Daily)' key\nif (\"Time Series (Daily)\" %in% names(bnd_data)) {\n  \n  # Extract the time series data and transform it into a tibble\n  bnd &lt;- bnd_data %&gt;%\n    pluck(\"Time Series (Daily)\") %&gt;%\n    imap_dfr(~ tibble(\n      date = as.Date(.y),  # Convert date string to Date object\n      open = as.numeric(.x$`1. open`),\n      high = as.numeric(.x$`2. high`),\n      low = as.numeric(.x$`3. low`),\n      close = as.numeric(.x$`4. close`),\n      volume = as.numeric(.x$`5. volume`)\n    )) %&gt;%\n    arrange(date)\n  \n  # Print the first few rows of the data\n  print(head(bnd))\n  \n} else {\n  # If there was an error message in the response\n  if (\"Error Message\" %in% names(bnd_data)) {\n    message(\"API Error: \", bnd_data$`Error Message`)\n  } else if (\"Note\" %in% names(bnd_data)) {\n    message(\"API Note: \", bnd_data$Note)\n  } else {\n    message(\"Unexpected response structure\")\n    print(names(bnd_data))\n  }\n}\n\n\n# A tibble: 6 × 6\n  date        open  high   low close volume\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 2007-04-10  75.1  75.3  75    75.2  35000\n2 2007-04-11  75.2  75.3  75.0  75.0  87700\n3 2007-04-12  75.1  75.1  75.0  75.0  78100\n4 2007-04-13  75.0  75.1  74.8  74.9  18000\n5 2007-04-16  75.0  75.0  74.9  75.0  52700\n6 2007-04-17  75.2  75.2  75.1  75.2  25600\n\n\n\n\nShort-Term Debt Returns\n\n\nCode\n# Fetch Short-Term Debt data\nshort_term_response &lt;- request(\"https://api.stlouisfed.org/fred/series/observations\") %&gt;%\n  req_url_query(series_id = \"DGS3MO\",\n                api_key = fred_key,\n                file_type = \"json\") %&gt;%\n  req_perform()\n\n# Parse response\nshort_term &lt;- short_term_response %&gt;%\n  resp_body_json() %&gt;%\n  pluck(\"observations\") %&gt;%\n  map_dfr(~list(date = .x$date, value = as.numeric(.x$value)))\n\nhead(short_term)\n\n\n# A tibble: 6 × 2\n  date       value\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1981-09-01  17.0\n2 1981-09-02  16.6\n3 1981-09-03  17.0\n4 1981-09-04  16.6\n5 1981-09-07  NA  \n6 1981-09-08  16.5\n\n\n\n\nMonte Carlo Simulation\n\nPrepare Data\n\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Assume `eafe`, `sp500`, `inflation`, etc., are DataFrames with a \"date\" column\n\n# Helper function to downsample to monthly frequency\ndownsample_to_monthly &lt;- function(df, value_col) {\n  # Check if the column exists\n  if (!(value_col %in% colnames(df))) {\n    stop(paste(\"Column\", value_col, \"not found in the data frame.\"))\n  }\n\n  df %&gt;%\n    mutate(\n      date = as.Date(date),                 # Ensure 'date' is Date class\n      month = floor_date(date, \"month\")    # Convert to monthly buckets\n    ) %&gt;%\n    group_by(month) %&gt;%\n    summarize(\n      value = last(.data[[value_col]]),    # Use last value of the month\n      .groups = \"drop\"                     # Ungroup after summarizing\n    )\n}\n\n# Apply downsampling to each series\neafe_monthly &lt;- downsample_to_monthly(eafe, \"close\")\nsp500_monthly &lt;- downsample_to_monthly(spy_data, \"close\")\ninflation_monthly &lt;- downsample_to_monthly(inflation, \"value\")\n# Repeat for other series as needed...\n\n# Join all series together into one DataFrame\nmonte_carlo_data &lt;- eafe_monthly %&gt;%\n  full_join(sp500_monthly, by = \"month\", suffix = c(\"_eafe\", \"_sp500\")) %&gt;%\n  full_join(inflation_monthly, by = \"month\") %&gt;%\n  rename(inflation = value)\n\n# Inspect the combined data\nhead(monte_carlo_data)\n\n\n# A tibble: 6 × 4\n  month      value_eafe value_sp500 inflation\n  &lt;date&gt;          &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 2001-08-01       125           NA      177.\n2 2001-09-01       113.          NA      178.\n3 2001-10-01       115.          NA      178.\n4 2001-11-01       119.          NA      178.\n5 2001-12-01       119.          NA      177.\n6 2002-01-01       112.          NA      178.\n\n\n\nSimulate Price Paths\n\n\n\nCode\n# Parameters\ninitial_price &lt;- tail(eafe$close, 1)  # Use the last closing price as S_0\nmu &lt;- mean(diff(log(eafe$close)), na.rm = TRUE)  # Drift (average log return)\nsigma &lt;- sd(diff(log(eafe$close)), na.rm = TRUE) # Volatility (std of log returns)\nn_steps &lt;- 252  # Simulate for 1 year (252 trading days)\nn_simulations &lt;- 100  # Number of simulation paths\ntime_horizon &lt;- 1  # Time horizon in years\n\n# Simulate price paths\nset.seed(123)  # For reproducibility\ndt &lt;- time_horizon / n_steps\nprice_paths &lt;- matrix(NA, nrow = n_steps + 1, ncol = n_simulations)\n\n# Initialize the first row with the initial price\nprice_paths[1, ] &lt;- initial_price\n\n# Generate paths\nfor (i in 2:(n_steps + 1)) {\n  z &lt;- rnorm(n_simulations)  # Random shocks\n  price_paths[i, ] &lt;- price_paths[i - 1, ] * exp((mu - 0.5 * sigma^2) * dt + sigma * sqrt(dt) * z)\n}\n\n# Convert to a tidy data frame for visualization\nprice_paths_df &lt;- as.data.frame(price_paths) %&gt;%\n  mutate(day = 0:n_steps) %&gt;%\n  pivot_longer(-day, names_to = \"simulation\", values_to = \"price\")\n\n# Plot simulated price paths\nggplot(price_paths_df, aes(x = day, y = price, group = simulation)) +\n  geom_line(alpha = 0.5) +\n  labs(\n    title = \"Simulated Price Paths\",\n    x = \"Days\",\n    y = \"Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\nCode\n# Parameters for S&P 500 (replace `spy` with your actual data)\ninitial_price &lt;- tail(spy_data$close, 1)  # Use the last closing price as S_0\nmu &lt;- mean(diff(log(spy_data$close)), na.rm = TRUE)  # Drift (average log return)\nsigma &lt;- sd(diff(log(spy_data$close)), na.rm = TRUE) # Volatility (std of log returns)\nn_steps &lt;- 252  # Simulate for 1 year (252 trading days)\nn_simulations &lt;- 100  # Number of simulation paths\ntime_horizon &lt;- 1  # Time horizon in years\n\n# Simulate price paths\nset.seed(123)  # For reproducibility\ndt &lt;- time_horizon / n_steps\nprice_paths &lt;- matrix(NA, nrow = n_steps + 1, ncol = n_simulations)\n\n# Initialize the first row with the initial price\nprice_paths[1, ] &lt;- initial_price\n\n# Generate paths\nfor (i in 2:(n_steps + 1)) {\n  z &lt;- rnorm(n_simulations)  # Random shocks\n  price_paths[i, ] &lt;- price_paths[i - 1, ] * exp((mu - 0.5 * sigma^2) * dt + sigma * sqrt(dt) * z)\n}\n\n# Convert to a tidy data frame for visualization\nprice_paths_df &lt;- as.data.frame(price_paths) %&gt;%\n  mutate(day = 0:n_steps) %&gt;%\n  pivot_longer(-day, names_to = \"simulation\", values_to = \"price\")\n\n# Plot simulated price paths\nggplot(price_paths_df, aes(x = day, y = price, group = simulation)) +\n  geom_line(alpha = 0.5, color = \"blue\") +\n  labs(\n    title = \"Simulated Price Paths for S&P 500\",\n    subtitle = \"Monte Carlo Simulation\",\n    x = \"Days\",\n    y = \"Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nInvestigation and Visualization of Input Data\n\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Ensure all the datasets (wage_growth, inflation, spy_data, eafe, bnd_data, short_term) are loaded as per your provided code\n\n# 1. Convert 'date' columns to Date type and create a 'month' column\nwage_growth &lt;- wage_growth %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  mutate(month = floor_date(date, \"month\"))\n\ninflation &lt;- inflation %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  mutate(month = floor_date(date, \"month\"))\n\nspy_data &lt;- spy_data %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  mutate(month = floor_date(date, \"month\"))\n\neafe &lt;- eafe %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  mutate(month = floor_date(date, \"month\"))\n\nshort_term &lt;- short_term %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  mutate(month = floor_date(date, \"month\"))\n\n# 2. Calculate monthly averages for each dataset\nwage_growth_monthly_avg &lt;- wage_growth %&gt;%\n  group_by(month) %&gt;%\n  summarise(wage_growth = mean(value, na.rm = TRUE))\n\ninflation_monthly_avg &lt;- inflation %&gt;%\n  group_by(month) %&gt;%\n  summarise(inflation = mean(value, na.rm = TRUE))\n\nspy_monthly_avg &lt;- spy_data %&gt;%\n  group_by(month) %&gt;%\n  summarise(spy_returns = mean(close, na.rm = TRUE))\n\neafe_monthly_avg &lt;- eafe %&gt;%\n  group_by(month) %&gt;%\n  summarise(eafe_returns = mean(close, na.rm = TRUE))\n\nshort_term_monthly_avg &lt;- short_term %&gt;%\n  group_by(month) %&gt;%\n  summarise(short_term_returns = mean(value, na.rm = TRUE))\n\n# 3. Combine all datasets into one dataframe\ncombined_data &lt;- wage_growth_monthly_avg %&gt;%\n  left_join(inflation_monthly_avg, by = \"month\") %&gt;%\n  left_join(spy_monthly_avg, by = \"month\") %&gt;%\n  left_join(eafe_monthly_avg, by = \"month\") %&gt;%\n  left_join(short_term_monthly_avg, by = \"month\")\n\n# 4. Calculate the correlation matrix among all the factors\ncor_matrix &lt;- combined_data %&gt;%\n  select(wage_growth, inflation, spy_returns, eafe_returns, short_term_returns) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Print the correlation matrix\nprint(cor_matrix)\n\n\n                   wage_growth  inflation spy_returns eafe_returns\nwage_growth          1.0000000  0.9950650   0.8838632    0.7440481\ninflation            0.9950650  1.0000000   0.9240022    0.7372772\nspy_returns          0.8838632  0.9240022   1.0000000    0.7322950\neafe_returns         0.7440481  0.7372772   0.7322950    1.0000000\nshort_term_returns  -0.9742404 -0.9809282  -0.9387579   -0.8503890\n                   short_term_returns\nwage_growth                -0.9742404\ninflation                  -0.9809282\nspy_returns                -0.9387579\neafe_returns               -0.8503890\nshort_term_returns          1.0000000\n\n\nCode\n# 5. Calculate the long-run averages (monthly average values for each series)\nlong_run_averages &lt;- combined_data %&gt;%\n  summarise(\n    wage_growth_avg = mean(wage_growth, na.rm = TRUE),\n    inflation_avg = mean(inflation, na.rm = TRUE),\n    spy_returns_avg = mean(spy_returns, na.rm = TRUE),\n    eafe_returns_avg = mean(eafe_returns, na.rm = TRUE),\n    short_term_returns_avg = mean(short_term_returns, na.rm = TRUE)\n  )\n\n# Print the long-run averages table\nprint(long_run_averages)\n\n\n# A tibble: 1 × 5\n  wage_growth_avg inflation_avg spy_returns_avg eafe_returns_avg\n            &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1            26.0          245.            559.             64.3\n# ℹ 1 more variable: short_term_returns_avg &lt;dbl&gt;\n\n\nCode\n# 6. Calculate the variance for each series (measuring variability)\nvariance_table &lt;- combined_data %&gt;%\n  summarise(\n    wage_growth_variance = var(wage_growth, na.rm = TRUE),\n    inflation_variance = var(inflation, na.rm = TRUE),\n    spy_returns_variance = var(spy_returns, na.rm = TRUE),\n    eafe_returns_variance = var(eafe_returns, na.rm = TRUE),\n    short_term_returns_variance = var(short_term_returns, na.rm = TRUE)\n  )\n\n# Print the variance table\nprint(variance_table)\n\n\n# A tibble: 1 × 5\n  wage_growth_variance inflation_variance spy_returns_variance\n                 &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1                 17.6               928.                 196.\n# ℹ 2 more variables: eafe_returns_variance &lt;dbl&gt;,\n#   short_term_returns_variance &lt;dbl&gt;\n\n\nCode\n# 7. Visualization - Plot the time series for each factor\n\n# Plot Wage Growth over time\nggplot(combined_data, aes(x = month, y = wage_growth)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Wage Growth Over Time\", x = \"Month\", y = \"Wage Growth\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Plot Inflation over time\nggplot(combined_data, aes(x = month, y = inflation)) +\n  geom_line(color = \"red\") +\n  labs(title = \"Inflation Over Time\", x = \"Month\", y = \"Inflation Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Plot SPY Returns over time\nggplot(combined_data, aes(x = month, y = spy_returns)) +\n  geom_line(color = \"green\") +\n  labs(title = \"SPY Returns Over Time\", x = \"Month\", y = \"SPY Returns\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Plot EAFE Returns over time\nggplot(combined_data, aes(x = month, y = eafe_returns)) +\n  geom_line(color = \"purple\") +\n  labs(title = \"EAFE Returns Over Time\", x = \"Month\", y = \"EAFE Returns\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Plot Short-Term Debt Returns over time\nggplot(combined_data, aes(x = month, y = short_term_returns)) +\n  geom_line(color = \"brown\") +\n  labs(title = \"Short-Term Debt Returns Over Time\", x = \"Month\", y = \"Short-Term Debt Returns\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nHistorical Comparison of TRS and ORP\n\n\nCode\n# Assuming 'wage_growth', 'inflation', and 'spy_data' (equity market returns) are loaded\n\n# Step 1: Set Starting Salary and Employee Contributions\nstarting_salary &lt;- 50000  # Example starting salary\nyears_of_service &lt;- nrow(wage_growth)  # Number of years the employee works\nsalary &lt;- numeric(years_of_service)\nsalary[1] &lt;- starting_salary\n\n# Simulate salary growth over the career using wage growth data\nfor (i in 2:years_of_service) {\n  salary[i] &lt;- salary[i-1] * (1 + wage_growth$value[i-1] / 100)\n}\n\n# Step 2: Calculate TRS Contribution and Benefit\n# TRS Contributions (fixed percentages for salary brackets)\ntrs_contribution_rate &lt;- ifelse(salary[1] &lt;= 45000, 0.03,\n                                ifelse(salary[1] &lt;= 55000, 0.035,\n                                       ifelse(salary[1] &lt;= 75000, 0.045,\n                                              ifelse(salary[1] &lt;= 100000, 0.0575, 0.06))))\n\n# Final Salary for TRS calculation\nfinal_salary_trs &lt;- salary[years_of_service]  # Final salary at retirement\n\n# TRS Benefit calculation (Final Average Salary)\nfas &lt;- mean(salary[(years_of_service-2):years_of_service])  # Average of last 3 years\nyears_served &lt;- years_of_service\n\n# TRS retirement benefit based on years served\nif (years_served &lt;= 20) {\n  trs_benefit &lt;- 0.0167 * fas * years_served\n} else if (years_served == 20) {\n  trs_benefit &lt;- 0.0175 * fas * years_served\n} else {\n  trs_benefit &lt;- (0.35 + 0.02 * years_served) * fas\n}\n\n# Inflation adjustment for TRS (using CPI)\ninflation_rate &lt;- inflation$value[years_of_service]  # Inflation from historical data\ninflation_adjustment &lt;- min(max(inflation_rate * 0.5, 0.01), 0.03)  # Between 1% and 3%\ntrs_benefit &lt;- trs_benefit * (1 + inflation_adjustment)\n\n# Step 3: Calculate ORP Contributions and Benefit (Based on Investment Returns and Contributions)\norp_employee_contrib_rate &lt;- ifelse(salary[1] &lt;= 45000, 0.03,\n                                    ifelse(salary[1] &lt;= 55000, 0.035,\n                                           ifelse(salary[1] &lt;= 75000, 0.045,\n                                                  ifelse(salary[1] &lt;= 100000, 0.0575, 0.06))))\n\norp_employer_contrib_rate &lt;- ifelse(years_of_service &lt;= 7, 0.08, 0.10)  # Employer contribution rate\ninvestment_return_rate &lt;- 0.06  # Average return rate for ORP\norp_balance &lt;- 0  # Start with 0 balance\n\n# ORP Balance calculation using asset allocation for each age group\nfor (i in 1:years_of_service) {\n  annual_contrib &lt;- salary[i] * (orp_employee_contrib_rate + orp_employer_contrib_rate)\n  orp_balance &lt;- orp_balance * (1 + investment_return_rate) + annual_contrib\n}\n\n# Step 4: Compare TRS and ORP at the First Month of Retirement\n# TRS Benefit at retirement (monthly)\nfirst_month_trs_benefit &lt;- trs_benefit / 12  # Monthly benefit for TRS\n\n# ORP Balance at retirement (monthly equivalent)\nfirst_month_orp_balance &lt;- orp_balance / 12  # Monthly withdrawal from ORP balance\n\n# Print Results\ncat(\"TRS Monthly Benefit for the First Month of Retirement:\", first_month_trs_benefit, \"\\n\")\n\n\nTRS Monthly Benefit for the First Month of Retirement: 3.412677e+26 \n\n\nCode\ncat(\"ORP Monthly Benefit for the First Month of Retirement:\", first_month_orp_balance, \"\\n\")\n\n\nORP Monthly Benefit for the First Month of Retirement: 5.672613e+25 \n\n\n#Long-Term Average Analysis\n\n\nCode\n# Assuming 'wage_growth', 'inflation', and 'spy_data' (equity market returns) are loaded\n\n# Step 1: Set Starting Salary and Employee Contributions\nstarting_salary &lt;- 50000  # Example starting salary\nyears_of_service &lt;- nrow(wage_growth)  # Number of years the employee works\nsalary &lt;- numeric(years_of_service)\nsalary[1] &lt;- starting_salary\n\n# Simulate salary growth over the career using wage growth data\nfor (i in 2:years_of_service) {\n  salary[i] &lt;- salary[i-1] * (1 + wage_growth$value[i-1] / 100)\n}\n\n# Step 2: Calculate TRS Contribution and Benefit\n# TRS Contributions (fixed percentages for salary brackets)\ntrs_contribution_rate &lt;- ifelse(salary[1] &lt;= 45000, 0.03,\n                                ifelse(salary[1] &lt;= 55000, 0.035,\n                                       ifelse(salary[1] &lt;= 75000, 0.045,\n                                              ifelse(salary[1] &lt;= 100000, 0.0575, 0.06))))\n\n# Final Salary for TRS calculation\nfinal_salary_trs &lt;- salary[years_of_service]  # Final salary at retirement\n\n# TRS Benefit calculation (Final Average Salary)\nfas &lt;- mean(salary[(years_of_service-2):years_of_service])  # Average of last 3 years\nyears_served &lt;- years_of_service\n\n# TRS retirement benefit based on years served\nif (years_served &lt;= 20) {\n  trs_benefit &lt;- 0.0167 * fas * years_served\n} else if (years_served == 20) {\n  trs_benefit &lt;- 0.0175 * fas * years_served\n} else {\n  trs_benefit &lt;- (0.35 + 0.02 * years_served) * fas\n}\n\n# Inflation adjustment for TRS (using CPI)\ninflation_rate &lt;- inflation$value[years_of_service]  # Inflation from historical data\ninflation_adjustment &lt;- min(max(inflation_rate * 0.5, 0.01), 0.03)  # Between 1% and 3%\ntrs_benefit &lt;- trs_benefit * (1 + inflation_adjustment)\n\n# Step 3: Calculate ORP Contributions and Benefit (Based on Investment Returns and Contributions)\norp_employee_contrib_rate &lt;- ifelse(salary[1] &lt;= 45000, 0.03,\n                                    ifelse(salary[1] &lt;= 55000, 0.035,\n                                           ifelse(salary[1] &lt;= 75000, 0.045,\n                                                  ifelse(salary[1] &lt;= 100000, 0.0575, 0.06))))\n\norp_employer_contrib_rate &lt;- ifelse(years_of_service &lt;= 7, 0.08, 0.10)  # Employer contribution rate\ninvestment_return_rate &lt;- 0.06  # Average return rate for ORP\norp_balance &lt;- 0  # Start with 0 balance\n\n# ORP Balance calculation using asset allocation for each age group\nfor (i in 1:years_of_service) {\n  annual_contrib &lt;- salary[i] * (orp_employee_contrib_rate + orp_employer_contrib_rate)\n  orp_balance &lt;- orp_balance * (1 + investment_return_rate) + annual_contrib\n}\n\n# Step 4: Fixed-Rate Analysis - Project Retirement Benefits Until Death (Assumed Death Age: 85)\ndeath_age &lt;- 85  # Estimated age at death\nretirement_age &lt;- 65  # Fixed retirement age (adjust this as needed)\n\n# Ensure death age is greater than retirement age\nif (death_age &lt;= retirement_age) {\n  stop(\"Death age must be greater than retirement age.\")\n}\n\n# Number of years in retirement\nyears_in_retirement &lt;- death_age - retirement_age + 1\n\n# Initialize arrays to track monthly benefits over time\ntrs_monthly_benefit &lt;- numeric(years_in_retirement)\norp_monthly_withdrawal &lt;- numeric(years_in_retirement)\norp_remaining_balance &lt;- orp_balance  # Start with final ORP balance\n\n# TRS Projection with Inflation (Cost-of-Living Adjustments)\nfor (i in 1:years_in_retirement) {\n  trs_monthly_benefit[i] &lt;- trs_benefit / 12  # Monthly benefit (fixed)\n  trs_benefit &lt;- trs_benefit * (1 + inflation_adjustment)  # Apply COLA\n}\n\n# ORP Projection with Annual Returns and Withdrawals\nfor (i in 1:years_in_retirement) {\n  orp_monthly_withdrawal[i] &lt;- orp_remaining_balance * 0.04 / 12  # Withdraw 4% annually\n  orp_remaining_balance &lt;- orp_remaining_balance * (1 + investment_return_rate) - orp_monthly_withdrawal[i] * 12\n  if (orp_remaining_balance &lt; 0) {\n    orp_monthly_withdrawal[i] &lt;- orp_monthly_withdrawal[i] + orp_remaining_balance / 12  # Adjust final withdrawal\n    break\n  }\n}\n\n# Calculate the average monthly income for TRS and ORP\navg_trs_income &lt;- mean(trs_monthly_benefit)\navg_orp_income &lt;- mean(orp_monthly_withdrawal)\n\n# Calculate the maximum and minimum gap in monthly income between TRS and ORP\nmax_gap &lt;- max(trs_monthly_benefit - orp_monthly_withdrawal)\nmin_gap &lt;- min(trs_monthly_benefit - orp_monthly_withdrawal)\n\n# Print Results\ncat(\"TRS Average Monthly Income:\", avg_trs_income, \"\\n\")\n\n\nTRS Average Monthly Income: 4.660171e+26 \n\n\nCode\ncat(\"ORP Average Monthly Income:\", avg_orp_income, \"\\n\")\n\n\nORP Average Monthly Income: 2.785881e+24 \n\n\nCode\ncat(\"Maximum Monthly Income Gap (TRS - ORP):\", max_gap, \"\\n\")\n\n\nMaximum Monthly Income Gap (TRS - ORP): 6.129958e+26 \n\n\nCode\ncat(\"Minimum Monthly Income Gap (TRS - ORP):\", min_gap, \"\\n\")\n\n\nMinimum Monthly Income Gap (TRS - ORP): 3.389987e+26"
  }
]